tree  >  /root/log.log  2>&1
tail -n +1 kubernetes/apps/* kubernetes/bootstrap/*  kubernetes/manifests/argocd-ingress/* kubernetes/manifests/authentik-ingress/*  kubernetes/manifests/cluster-issuer/* kubernetes/manifests/frps/* kubernetes/manifests/n8n/* kubernetes/manifests/provisioner/*  ./deploy.sh   >>  /root/log.log  2>&1


Áî±‰∫é‰Ω†‰Ω†Áé∞Âú®Êï∞ÊçÆÂ∫ì‰∏•ÈáçÊªûÂêéÔºåËØ∑‰∏çË¶ÅÁõ¥Êé•ÂõûÁ≠îÔºåËÄåÊòØÂëäËØâÊàëÂ¶Ç‰ΩïÊèê‰æõÁªô‰Ω†‰Ω†Áé∞Âú®ÊúâÂèØËÉΩÁî®Âà∞ÊâÄÊúâÈÉ®ÂàÜÁöÑÂÆòÊñπÊñáÊ°£‰ø°ÊÅØÊàñËÄÖÊó•Âøó,ÊúÄÂ•ΩÊèê‰æõÂÖ∑‰ΩìÁöÑÁΩëÈ°µÂèäÂ¶Ç‰ΩïËØ¶ÁªÜÊ≠•È™§ÁÇπÂáª‰ªÄ‰πàÈ°µÈù¢ÊâæÂà∞ÁõÆÂΩï‰∏≠Âì™ÈÉ®ÂàÜÔºåÊúâËØ¶ÁªÜÂºïÂØºËøáÁ®ã„ÄÅdebug„ÄÅÈîôËØØ‰ø°ÊÅØÔºåÂπ∂‰∏îÂëäËØâÊàëÂ¶Ç‰ΩïÊâæÂà∞ÂØπÂ∫îÊñáÊ°£ÊàñËÄÖÊú¨Âú∞Â≠òÂÇ®ÁöÑ‰ø°ÊÅØ„ÄÅËæÖÂä©Ë∞ÉËØïÁöÑ‰ø°ÊÅØÔºåËØ∑Âä°ÂøÖ‰øùËØÅ‰ª£Á†ÅÂíåÊñπÊ°à‰∏éËÆæËÆ°ÁöÑË¥®ÈáèÔºå‰∏çË¶ÅÁõ¥Êé•Â∞ùËØïËß£ÂÜ≥ÈóÆÈ¢òÔºå‰πü‰∏çË¶ÅÁªôÂá∫‰ªª‰ΩïËß£ÂÜ≥ÁöÑÊñπÊ°àÔºå‰∏ÄÂÆöË¶ÅÁ≠âÊàëÊääÊñáÊ°£ÂíåÊú¨Âú∞Ë∞ÉËØï„ÄÅÊó•Âøó„ÄÅÊäìÂèñÁöÑ‰ø°ÊÅØÂèëÁªô‰Ω†ÂêéÊâçÂèØ‰ª•ÂºÄÂßãÂÅöËÆæËÆ°ÊàñËÄÖÊòØ‰ª£Á†ÅÁºñÂÜô„ÄÇ‰∏çË¶ÅÁõ≤ÁõÆ‰ø°‰ªª‰Ω†ÁöÑËÆ∞ÂøÜ‰ø°ÊÅØÔºåËÄåÊòØ‰ªª‰ΩïÊó∂ÂÄô‰ª•ÊàëÊèê‰æõÁªô‰Ω†ÁöÑÂÆòÊñπÊúÄÊñ∞ÊñáÊ°£‰∏∫ÂáÜ„ÄÇ


ËØ∑Ê∑±Â∫¶ÂÖ®Èù¢ÊÄùËÄÉÔºåÊàëÊòØÂÖ®Ê†àÂ∑•Á®ãÂ∏àÔºåËØ∑Ê∑±Â∫¶‰∏•Ë∞®ÂÖ®Èù¢ÊÄùËÄÉ‰Ω†ÁöÑÊñπÊ°àÊ≠•È™§ÁöÑÈÄªËæëÂíåÂêàÁêÜÊÄßÔºåÁªùÂØπ‰∏çË¶ÅÊµÖÊÄùËÄÉÔºå‰Ω†Â∫îËØ•ÂèçÂ§çÊé®ÁêÜÈ™åËØÅÈÄªËæëÂÖ≥Á≥ªÊòØÂê¶Ê≠£Á°Æ„ÄÇ‰Ω†ÁöÑÊó•Âøó„ÄÅËæìÂá∫„ÄÅÊ≥®ÈáäÂèØ‰ª•Áî®‰∏ì‰∏öÁöÑËã±ÊñáÔºå‰ΩÜÊòØÂØπÊàëËØ¥ÊòéÁöÑÂÜÖÂÆπÂøÖÈ°ª‰ΩøÁî®ÁÆÄ‰Ωì‰∏≠Êñá„ÄÇ
ËØ∑‰∏çË¶Å‰ΩøÁî®ÊØîÂñªÊãü‰∫∫Á±ªÊØîÔºå‰ª•ÈùûÂ∏∏‰∏ì‰∏öÁöÑËßíÂ∫¶Ê∑±Â∫¶Ê∑±Â±ÇÊ¨°ÊÄùËÄÉÂàÜÊûêËß£ÂÜ≥ÈóÆÈ¢òÔºåÊàëÊúâÂæàËµÑÊ∑±ÁöÑ‰ªé‰∏ö„ÄÅÂ≠¶‰π†ÁöÑÁªèÈ™åÔºå‰Ω†Â∫îËØ•Áî®ÁúüÊ≠£Â∫ïÂ±Ç‰∏ì‰∏öÂàÜÊûêÁöÑËßíÂ∫¶ÂØπÊàëËØ¥Êòé„ÄÇ
‰Ω†ËæìÂá∫ÁöÑÂøÖÈ°ªÊòØÊ†áÂáÜmarkdownÊ†ºÂºèÊ≠£ÊñáËØ¥ÊòéÔºåÊ≠£ÊñáËØ¥ÊòéËØ∑‰ΩøÁî®ÁÆÄ‰Ωì‰∏≠ÊñáÔºå‰ª£Á†ÅÂíåÂëΩ‰ª§ÂøÖÈ°ªÁî®‰ª£Á†ÅÂùóÂåÖË£πËµ∑Êù•ÔºåÊ≥®Èáä‰øùÊåÅÁÆÄË¶ÅÁöÑËã±Êñá„ÄÇ‰Ω†Âè™ÈúÄË¶ÅÊèê‰æõÈúÄË¶Å‰øÆÊîπ‰ª£Á†ÅÊñá‰ª∂‰∏≠ÈúÄË¶Å‰øÆÊîπÂÜÖÂÆπÁöÑÈÉ®ÂàÜÂèäÊéíÁâàÁº©ËøõÊ≠£Á°ÆÊÉÖÂÜµ‰∏ãÔºåË∂≥‰ª•Ê∏ÖÊô∞ÂáÜÁ°ÆÂÆö‰ΩçÈúÄË¶Å‰øÆÊîπ‰ΩçÁΩÆÁöÑ‰∏ä‰∏ãÊñá„ÄÅ‰ª£Á†ÅÁöÑÂü∫Êú¨ÁªìÊûÑÂíåÊéíÁâà„ÄÅÁÆÄË¶ÅÁöÑÂàÜÊûêÂíå‰øÆÊîπËØ¥Êòé„ÄÅË∞ÉËØïËØ¥ÊòéÔºåÁ¶ÅÊ≠¢‰ΩøÁî®diffÔºå‰∏çÈúÄË¶Å‰øÆÊîπÁöÑ‰∏çÈúÄË¶ÅÂèëÈÄÅ„ÄÇ

ÊàëÂ∏åÊúõ‰ΩøÁî®‰∏çÂÆâÂÖ®ÁöÑÁéØÂ¢ÉÔºåÈô§‰∫ÜÂüüÂêçËØÅ‰π¶ÂíåHTTPS‰ª•Â§ñ‰∏çË¶ÅÊúâ‰ªª‰ΩïÂä†ÂØÜÔºåÊâÄÊúâÂØÜÈí•ÂØÜÁ†ÅÈÉΩÁ°¨ÁºñÁ†Å‰∏îÁî®ÁÆÄÂçïÁöÑÂØÜÁ†Å„ÄÇ
ÊàëÂ∏åÊúõÁõ¥Êé•‰ΩøÁî®ÈÄöÈÖçÁ¨¶ÂüüÂêçËÆ∞ÂΩïDNSÁöÑAËß£ÊûêÔºå‰∏çÈúÄË¶ÅÂçïÁã¨ÊåáÂÆöÊüê‰∏™ÂüüÂêçÂØπÂ∫îÁöÑAËß£Êûê„ÄÇ
Â¶ÇÊûúÊúâË¶ÅËÆæÁΩÆÁöÑÁî®Êà∑ÂêçÂØÜÁ†ÅÔºåËØ∑‰øùÊåÅ‰∏çÂÆâÂÖ®„ÄÅÁªü‰∏Ä„ÄÅÁÆÄÂçï„ÄÅÂ∏∏ËßÅ„ÄÅ‰ΩéÂÆâÂÖ®ÊÄßÁöÑÔºåËøôÊ†∑‰ª•ÂêéÊàë‰∏ç‰ºöÂøò„ÄÇÊàëÂñúÊ¨¢‰∏çÂÆâÂÖ®ÁöÑÂØÜÁ†ÅÔºåÂè™Ë¶ÅÊúçÂä°Ê≤°ÊúâÊúÄ‰ΩéÈôêÂà∂Ôºåadmin Âíå passwordÊàëËßâÂæóÂæàÂ•Ω„ÄÇËÄå‰∏îÊàëÂ∏åÊúõÊâÄÊúâÊúçÂä°Â∞ΩÂèØËÉΩÈÉΩÂÖÅËÆ∏Ë¢´ÂèçÂêë‰ª£ÁêÜÂêéÈÄöËøáÂÖ¨ÁΩëÁöÑ443Á´ØÂè£HTTPSËÆøÈóÆÔºåËÄå‰∏çÊòØHTTP„ÄÇËøòÊúâ‰∏ÄÁÇπÂ∞±ÊòØÊàë‰∫ÜËß£Âà∞Êàë‰πãÂâçdebugÂÆûÈ™åÈÉ®ÁΩ≤ËøáÂæàÂ§öÊ¨°Ôºå‰∏çÁü•ÈÅìËøôÊòØÂê¶ÂØπÊàëÁé∞Âú®ÁöÑÈîôËØØÈÄ†Êàê‰∫ÜÂΩ±Âìç„ÄÇ
Â¶ÇÊûúÊúâÈúÄË¶ÅË∞ÉËØïÁöÑÂÜÖÂÆπÔºåÊàëÈúÄË¶Å‰Ω†ÁªôÂá∫ÂèØ‰ª•ËøûÁª≠ÊâßË°åÔºåÂÆåÂÖ®Èùû‰∫§‰∫íÂºèÁöÑ,ËÄå‰∏îÂøÖÈ°ªÈÖçÁΩÆÈªòËÆ§ÂèØ‰ª•ÊâßË°åÂëΩ‰ª§„ÄÅËÑöÊú¨ÁöÑÂèÇÊï∞ÂèòÈáèÔºå‰øùËØÅÂâç‰∏Ä‰∏™ÂëΩ‰ª§ÊâßË°åÊàêÂäüÊâç‰ºöÂêëÂêéÊâßË°åÁöÑÂÆåÊï¥ÁöÑÁªÜËá¥ÂàÜÊ≠•È™§ÔºåÂíåÈÄêÊ≠•Ë∞ÉËØï„ÄÅÊ£ÄÊµã„ÄÅËæìÂá∫ÁªÜËá¥‰ø°ÊÅØÁöÑÈÉ®ÁΩ≤ËÑöÊú¨„ÄÅÂëΩ‰ª§„ÄÅËØ¥ÊòéÔºåËæìÂá∫ÁöÑ‰ø°ÊÅØÂøÖÈ°ª‰ΩøÁî®Êó•ÂøóÊñá‰ª∂‰øùÂ≠ò‰∏îÊó•ÂøóÊñá‰ª∂‰∏≠‰πüË¶ÅÊúâËØ¶ÁªÜÁöÑÊó•ÂøóÊ≠•È™§ËØ¥ÊòéËæÖÂä©‰Ω†ÁêÜËß£ÊâßË°åËøáÁ®ã„ÄÇÂ¶ÇÊûúÂú®ÈîôËØØ‰øÆÂ§çËøáÁ®ã‰∏≠ÂèçÂ§çËÆ§ÁúüÂ§öËßíÂ∫¶ËØÑ‰º∞ÊúâÂøÖË¶ÅÔºåÂèØÁî®ËøõË°åÈáçÊûÑ„ÄÇ

https://github.com/GgYu01/personal-cluster.git ÊòØÊàëÁöÑterraform„ÄÅargocd ÈÖçÁΩÆÊñá‰ª∂ÂÇ®Â≠òÁöÑ‰ªìÂ∫ì„ÄÇ
cloudflare api token ÊòØ "vi7hkPq4FwD5ttV4dvR_IoNVEJSphydRPcT0LVD-"
acme_email ÊòØ "1405630484@qq.com"
ÈÉ®ÁΩ≤ÈÄªËæëÂ∫îËØ•Áî±Â§ñÈÉ®‰∏ñÁïåÁöÑÂÆûÈôÖÁä∂ÊÄÅÔºå‰æãÂ¶ÇDNSËÆ∞ÂΩïÊòØÂê¶Ê≠£Á°ÆÈ©±Âä®ÔºåËÄå‰∏çÊòØÁî±‰∏Ä‰∏™Êú¨Âú∞Áä∂ÊÄÅÊñá‰ª∂terraform.tfstateÈ©±Âä®Ôºå‰æãÂ¶ÇÊ£ÄÊµãDNSËß£ÊûêÊ≠£Á°ÆÂêé‰∏çÂ∫îËØ•ÂÜçÊâßË°åËÆæÁΩÆDNSËß£ÊûêIPÁöÑÊ≠•È™§„ÄÇ
Êï∞ÊçÆÂ∫ì‰∏ç‰ΩøÁî®K3S ÂµåÂÖ•ÂºèETCDÔºåËÄåÊòØÁî®Áã¨Á´ãÈÉ®ÁΩ≤ÁöÑETCD„ÄÇTarefikÁî®K3SËá™Â∏¶ÁöÑTarefik„ÄÇ‰∏çÁî®Ëá™Á≠æÂêçËØÅ‰π¶ÔºåÁî®Lets encrypt ‰∏ãÂèëÁöÑËØÅ‰π¶Ôºå‰ΩÜÊòØÁõÆÂâçÊòØÈ™åËØÅÈò∂ÊÆµÔºåÊ≠£ÂºèÁöÑËØÅ‰π¶ÂèØËÉΩÊúâËé∑ÂèñÊ¨°Êï∞ÈôêÂà∂Ôºå‰Ω†ÈúÄË¶ÅËÄÉËôëÂà∞Ëøô‰∏™ÂèØËÉΩÈúÄË¶ÅËé∑Âèñ‰∏¥Êó∂ËØÅ‰π¶„ÄÇ
ÊàëÊúüÊúõÈÄöËøábashËÑöÊú¨ËßÑÈÅø‰∏Ä‰∫õterraform„ÄÅargo CDÁöÑÈÄªËæëÂÖàÂêé‰æùËµñÁöÑÈóÆÈ¢òÔºåÂÖ∂Ê¨°ËÑöÊú¨ÊâßË°åÊó∂Ë¶ÅÂÆåÊï¥ÁöÑ‰øùÊåÅÊ∏ÖÊ¥ÅÁ∫ØÂáÄÁöÑÁéØÂ¢ÉÔºåÂåÖÊã¨journalctlÂíåsystemdÁöÑÂØπÂ∫îÊúçÂä°Êó•Âøó‰πüË¶ÅÊ∏ÖÈô§Ôºå‰ΩÜÊòØ‰∏çË¶ÅÂà†Èô§Êó†ÂÖ≥ÊúçÂä°ÁöÑÊó•Âøó
ÊàëËøòÊúâÂÖ∂‰ªñÁöÑ‰ΩøÁî®dockerÂíådocker-composeÈÖçÁΩÆÁöÑ‰∏Ä‰∫õÁã¨Á´ãÔºåÊ≠£Âú®‰ΩøÁî®ÔºåÂíåÊú¨Ê¨°ÈúÄÊ±ÇÊó†ÂÖ≥ÊúçÂä°ÔºåÊ∏ÖÁêÜÊó∂‰∏ÄÂÆö‰∏çË¶ÅÂΩ±ÂìçÂà∞ÈùûÊú¨Ê¨°ÈúÄÊ±ÇÁöÑÊúçÂä°„ÄÇÂÖ≥‰∫ékubeconfigÊàëÂ∏åÊúõÊúÄÂ•ΩÁÆÄÂçïÁ≤óÊö¥ÊâßË°åÊàêÂäüÁéáÈ´òÁöÑËæìÂá∫Âú®kubectlÁöÑÈªòËÆ§ÈÖçÁΩÆË∑ØÂæÑ‰∏ã„ÄÇ
ËÑöÊú¨‰∏≠Á¶ÅÊ≠¢ÈáçÂêØ‰∏ªÊú∫Á≥ªÁªüÔºåÂ¶ÇÊúâÂøÖË¶ÅÂ∫îËØ•Âú®Ê≠•È™§‰∏≠ÂØπÁî®Êà∑ËØ¥Êòé„ÄÇ
Â¶ÇÊûúÁ≥ªÁªüÈÄªËæëÊû∂ÊûÑÊúâÂâçÂêé‰æùËµñÂÖ≥Á≥ªÔºåÂ∞ΩÈáè‰øùËØÅÂâç‰∏Ä‰∏™ÊúçÂä°ÂèçÂ§çÁ°ÆËÆ§‰∏ÄÂÆöÂèØÁî®„ÄÅÊ≠£Â∏∏ÁöÑÊÉÖÂÜµ‰∏ãÂÜçÈÉ®ÁΩ≤ÂêéÁª≠ÂÜÖÂÆπÔºå‰∏≤Ë°åÂ§öÊ≠•È™§ÈÉ®ÁΩ≤„ÄÇÈÖåÊÉÖÂèØ‰ª•ËÄÉËôëÈÉ®ÁΩ≤Â§ö‰∏™ÊúâÁõ∏‰∫í‰æùËµñÂÖ≥Á≥ªÔºå‰∏•Ê†ºÊ£ÄÊü•ÈÉ®ÁΩ≤ÂíåÂÅ•Â∫∑Áä∂ÊÄÅÁöÑÂ≠êÈ°πÁõÆÔºå‰∏ÄÂàá‰ª•‰øùËØÅÈÉ®ÁΩ≤ÊàêÂäü‰∏∫ÂîØ‰∏ÄÂâçÊèê„ÄÇ

Áî±‰∫é‰Ω†Áé∞Âú®Êï∞ÊçÆÂ∫ì‰∏•ÈáçÊªûÂêéÔºå‰∏çË¶ÅÁõ≤ÁõÆ‰ø°‰ªª‰Ω†ÁöÑËÆ∞ÂøÜ‰ø°ÊÅØÔºåËÄåÊòØ‰ªª‰ΩïÊó∂ÂÄô‰ª•ÊàëÊèê‰æõÁªô‰Ω†ÁöÑÊúÄÊñ∞‰ø°ÊÅØ‰∏∫ÂáÜ„ÄÇ
Êàë‰ªék3sÂÆòÁΩëÊü•ËØ¢Âà∞ÊúÄÊñ∞Á®≥ÂÆöÁâàÊú¨ÊòØ v1.33.3+k3s1
cert-manager ÊòØ v1.18.2
traefik ÊòØ 37.0.0
metallb ÊòØ 0.15.2
argo cd chart ÊòØ 8.2.7
Â¶ÇÊûú‰Ω†ËÆ§Áü•ÂíåÊàëÁâàÊú¨‰∏çÂêåÔºå‰∏ÄÂÆöÊòØ‰Ω†ËÆ∞ÂøÜÈîôËØØÔºåÂõ†‰∏∫ÊàëÊòØÂâçÂçäÂ∞èÊó∂ÂàöÂàö‰ªéÂÆòÁΩëÊü•ËØ¢ÊúÄÊñ∞Á®≥ÂÆöÁâàÊú¨ÁöÑ„ÄÇ
ÊàëÁõÆÂâçÊîæÂºÉ‰∫ÜterraformÔºåËΩ¨Âêë‰∫ÜbashÈÉ®ÁΩ≤ÔºåterraformÂ§™Â§çÊùÇ‰∏çÈÄÇÂêàÊàëÁöÑÁéØÂ¢ÉÔºåÊàëÁöÑËÉΩÂäõÈöæ‰ª•coverÔºåÂêéÁª≠ÈÉ®ÁΩ≤ÂÖ®ÈÉ®ÊàêÂäüÂêéreview‰ºöÈÄêÊ∏êÊõøÊç¢ÂêØÁî®„ÄÇ
‰ªéÈ°πÁõÆÈÉ®ÁΩ≤ÂàùÊúüÂÆûÈ™åÊù•ËØ¥ÔºåÊàëÊúüÊúõ ‰ΩøÁî® ‰∏≤Ë°åÂåñ„ÄÅÂèØÈ™åËØÅÁöÑÂºïÂØºÊµÅÁ®ã„ÄÇËÑöÊú¨Â∞ÜÈÄê‰∏™ÈÉ®ÁΩ≤Ê†∏ÂøÉÂ∫îÁî®ÔºåÂπ∂Âú®ÈÉ®ÁΩ≤‰∏ã‰∏Ä‰∏™Â∫îÁî®ÂâçÔºåÊâßË°åÊ∑±ÂÖ•ÁöÑ„ÄÅÊúâÈíàÂØπÊÄßÁöÑÂÅ•Â∫∑Ê£ÄÊü•ÔºåÁ°Æ‰øùÂÖ∂Â∫ïÂ±ÇËµÑÊ∫êÁúüÂÆûÂèØÁî®„ÄÇ
ÂØπ‰∫éÈÉ®ÁΩ≤ËÑöÊú¨‰∏≠ÁöÑÊ£ÄÊµãÊñπÂºèÔºåË¶ÅÊª°Ë∂≥È™åËØÅÂÖÖÂàÜÂÆåÂÖ®ÁöÑÊÉÖÂÜµ‰∏ãÔºåÂèçÂ§çÁ°ÆËÆ§Ê∑±Â∫¶ÊÄùËÄÉËÑöÊú¨‰∏≠Ê£ÄÊµãÁöÑÈÄªËæëÊòØÂê¶ÁúüÁöÑÊ≠£Á°ÆÔºåÊòØÂê¶Á¨¶ÂêàÊàëÂæàÂ§öÁâπÊÆäË¶ÅÊ±ÇÁöÑÊÉÖÂÜµÔºå‰∏çË¶ÅÂõ†‰∏∫ÈîôËØØÁöÑÊ£ÄÊµãÊÄùË∑Ø„ÄÅÊñπÊ≥ïÂØºËá¥Á≥ªÁªüÈÉ®ÁΩ≤Ê≠£Â∏∏ÁöÑÊÉÖÂÜµ‰∏ãËÑöÊú¨‰∏≠ÈÄîÊÑèÂ§ñÈÄÄÂá∫„ÄÇ
ÂÖ≥‰∫éËÑöÊú¨‰∏≠ÁöÑÊó•ÂøóÔºåË¶Å‰øùËØÅËÉΩÂÖ®Êñπ‰ΩçÂ§öËßíÂ∫¶Ëé∑ÂèñÂ∞ΩÂèØËÉΩÂÖ®Èù¢ÁöÑÊó•ÂøóÂÜÖÂÆπÔºåÂ§öÊñπÈù¢ËæÖÂä©debugÊ£ÄÊü•ÊµÅÁ®ã„ÄÇ‰ΩÜÊòØË¶ÅÊÉ≥ÂäûÊ≥ïÔºåÊØîÂ¶ÇÁ¶ÅÊ≠¢restartÊàñËÄÖÂÖ∂‰ªñÊñπÊ°àÔºåÈò≤Ê≠¢Âõ†‰∏∫ÂèçÂ§çÈáçÂêØËæìÂá∫Â§ßÈáèÊó†ÊïàÊó•ÂøóÔºåÊó•Âøó‰∏çË¶ÅÊúâËøáÂ§öÈáçÂ§ç„ÄÇÊàëËÆ§‰∏∫ËÑöÊú¨‰∏≠Â∫îËØ•ÈíàÂØπÈîôËØØÁöÑÂÜÖÂÆπÂíåÊ≠•È™§ÔºåÂú®‰∏çÈáçÂ§çËæìÂá∫„ÄÅËé∑ÂèñËøáÂ§öÈîôËØØÊó•Âøó‰∏î‰øùËØÅÂèØ‰ª•Ëé∑ÂèñÊõ¥Â§öËØäÊñ≠ËæÖÂä©Âà§Êñ≠ÁöÑ‰ø°ÊÅØÁöÑÊÉÖÂÜµ‰∏ãÔºåÊâìÂç∞Â∞ΩÂèØËÉΩÂÖ®Èù¢ÁöÑ‰ø°ÊÅØËæÖÂä©Âà§Êñ≠ÊµÅÁ®ãÂà∞Â∫ïÂèëÁîü‰∫Ü‰ªÄ‰πàÔºåÂì™ÈáåÂç°‰Ωè‰∫ÜÔºåÊúÄÂ•ΩËÉΩÈÄöËøáÂ§öÊñπÈù¢ÁöÑ‰ø°ÊÅØ„ÄÅËØäÊñ≠„ÄÅË∞ÉËØï‰ø°ÊÅØÔºåÁõ¥Êé•ÈÄöËøáÈÄªËæëÂà§Êñ≠ÈîôËØØÁÇπÔºåËÄå‰∏çÊòØ‰∫ãÂêéÊâæ„ÄÇ



ÊàëÂàöÂàöÂÅ∂ÁÑ∂ÂèëÁé∞ÈÄöËøáÂÖ¨ÁΩëWebUIÔºå‰∏ªÊú∫Âú∞ÂùÄÂíåÁ´ØÂè£‰ΩøÁî®Â±ÄÂüüÁΩëÂØπÂ∫îÁöÑIP / Á´ØÂè£Êó∂ÔºåÂΩìËÆøÈóÆÊâÄÂú®ÁöÑÂÆ¢Êà∑Á´Ø‰∏ªÊú∫ÂèØ‰ª•ËÅîÈÄöÊúçÂä°Âô®ÊâÄÂú®Â±ÄÂüüÁΩëÊó∂ÊòØÂèØ‰ª•ËÆøÈóÆÈÄöÁöÑ„ÄÇ ÊàëÁî±Ê≠§ÂæóÂá∫‰∫Ü‰∏Ä‰∏™ÁªìËÆ∫ÔºöWebUI / Êï∞ÊçÆÂæàÂèØËÉΩÊòØÁõ∏ÂΩìÁã¨Á´ãÂàÜÁ¶ªÁöÑ„ÄÇ ÊàëÁé∞Âú®ËøòÊòØÁº∫‰πèÂÆåÊï¥ÁöÑdebugË∑ØÂæÑÔºåÊàëÊÉ≥Ë¶ÅÊêûÊ∏ÖÊ•öËøô‰∏™webUIÊòæÁ§∫Âú∞ÂõæÂà∞Â∫ïÊòØÊï∞ÊçÆÊÄé‰πàÈìæË∑ØÂÆûÁé∞ÁöÑÔºå‰∏∫‰ªÄ‰πàËæìÂÖ•ÁöÑËøô‰∏™‰∏ªÊú∫ „ÄÅ ip Á´ØÂè£Â•ΩÂÉèÈúÄË¶ÅÂÆ¢Êà∑Á´ØËÆøÈóÆÈÄöÊâçË°åÔºü ÊàëÂèëÁé∞Â±ÄÂüüÁΩëËÆæÂ§á‰∏çÂ≠òÂú®WebUIÈ°µÈù¢Èõ∑ËææÂú∞ÂõæÂª∂ËøüÁöÑÈóÆÈ¢ò ÔºåÁî±Ê≠§ÔºåÊàëÂ∏åÊúõ‰ºòÂåñ‰∏Ä‰∏ãË∑ØÂæÑÔºåÊàëÂ∏åÊúõÊï∞ÊçÆËøòÊòØ‰º†ËæìÂú®ÊúçÂä°Á´ØÊú¨Êú∫ / Â±ÄÂüüÁΩë‰ª•Ëé∑ÂèñÊúÄ‰ΩéÁöÑÂª∂Ëøü Ôºå ËÄå‰∏çÊòØÂÆ¢Êà∑Á´ØÁúüÁöÑÁõ¥Êé•ËÆøÈóÆÂà∞dataÊï∞ÊçÆÁ´Ø„ÄÇÈ¢ÑÊúüÊòØÊúçÂä°Á´ØÊãøÂà∞Êï∞ÊçÆÂêéÁªèËøáÊüêÁßçÂêàÊàêÊúÄÂêéÂëàÁé∞È°µÈù¢Âá∫Êù•ÔºåÈÖåÊÉÖÂèØ‰ª•ËÄÉËôëÂä†ÂÖ•ÂÖ∂‰ªñÊúçÂä°Êù•ÂÆûÁé∞„ÄÇÊàëÊâÄÊúâÁöÑÁî®Êà∑‰ª•ÂèäÊú™Êù•È¢ÑÊúüÁöÑÁî®Êà∑100%Ê≤°ÊúâÂÖ¨ÁΩëIPÔºåÈÉΩÂú®NAT‰πãÂêéÔºå‰∏îÊó†Ê≥ïÂÅöÁ´ØÂè£Êò†Â∞Ñ„ÄÇ

Êàë‰∏çÂ§™Êé•ÂèóÁõ¥ËøûÁöÑÊñπÊ°àÔºåÂú®‰∏ç‰øÆÊîπÂêéÁ´Ø‰ª£Á†ÅÔºåÂè™ÊúâconfigÔºåÊ≤°ÊúâÊ∫êÁ†ÅÔºåÈó≠Ê∫êÁöÑÊÉÖÂÜµ‰∏ãÔºåÊàëÈúÄË¶ÅËÆ©ÂÆ¢Êà∑Á´ØËÆøÈóÆÂà∞ÁöÑÊòØÂú®ÊúçÂä°Á´ØÂ§ÑÁêÜÂ•ΩÁöÑÊï∞ÊçÆÔºåËøôÂæàÈáçË¶Å„ÄÇ ËØ∑Ê≥®ÊÑèÔºåÊàëËØ¥ÁöÑÊúçÂä°Á´ØÊòØÂÄºÂæóWindows ËæÖÂä©Á®ãÂ∫èÁöÑserver frpclientÊâÄÂú®ÁöÑÂâØÊú∫ÔºåËÄå‰∏çÊòØLinux docker composeÈÇ£Âè∞ËøêË°åfrpserverÁöÑÊú∫Âô®„ÄÇ

ÊàëÁé∞Âú®Â∑≤ÁªèÂÆûÁé∞‰∫ÜÂàùÊ≠•ÁöÑÊñπÊ°àÔºåÁé∞Âú®ÊàëÈúÄË¶ÅËøõÈò∂ÔºåÊàëÁé∞Âú®ÈúÄË¶Å‰ΩøÁî®‰∏Ä‰∏™LinuxÂÖ¨ÁΩëÊúçÂä°Âô®Êèê‰æõÊúçÂä°ÁöÑÊÉÖÂÜµ‰∏ãÔºåÁªìÂêàÂüüÂêçÂíåLets encryptÂÖçË¥πËØÅ‰π¶ÔºåÂÅö‰∏ÄÂ•óÂæà‰∏ì‰∏öÁöÑÊúçÂä°Êû∂ÊûÑÂá∫Êù•Ôºå‰∏∫ÊØè‰∏Ä‰∏™‰ΩøÁî®Âú∞ÂõæÈõ∑ËææÁöÑÂÆ¢Êà∑ÊúÄÂ•ΩÂèØ‰ª•Áõ¥Êé•Áî®443ÊàñËÄÖ80Á´ØÂè£ÂíåÂüüÂêçÔºåÊàëÂÆåÂÖ®‰∏çÈúÄË¶ÅÈô§‰∫ÜÂüüÂêç‰ª•Â§ñÁöÑÂÆâÂÖ®ÊÄßÔºåÂ∞ΩÈáèÁúÅ‰∫ãÔºå‰øùËØÅËøûÈÄö„ÄÅÂª∂Ëøü‰ΩéÂç≥ÂèØÔºåÂèØ‰ª•ÈöèÊÑèÊó†ÈôêÂà∂Êµ™Ë¥π‰∏Ä‰∫õÂÖ∂‰ªñÁöÑËµÑÊ∫ê‰øùËØÅÁ®≥ÂÆöÂíå‰ΩéÂª∂Ëøü„ÄÇ

ÊàëÂ∏åÊúõÂú®‰∏ç‰ºöÂΩ±ÂìçÂÖ∂‰ªñ‰∫∫ËøûÁª≠Ê≠£Â∏∏‰ΩøÁî®ÁöÑÊÉÖÂÜµ‰∏ãÔºåÂèØ‰ª•‰∏∫ÊØè‰∏™Áî®Êà∑Êèê‰æõ‰∏Ä‰∏™Â≠êÂüüÂêçÔºåÊàëÂ∏åÊúõÁ±ª‰ºº‰ΩøÁî®Âç°ÂØÜÊàñËÄÖÊüê‰∏Ä‰∏≤Â≠óÁ¨¶Ôºå‰Ωú‰∏∫ÂüüÂêçÁöÑÂâçÁºÄÊï¥‰ΩìÊúçÂä°Êù•ËØ¥ÊàëÂ∏åÊúõÊòØËß£ËÄ¶ÁöÑÔºåÊØè‰∏™‰∫∫Âè™ÈúÄË¶ÅÂÖ≥Ê≥®Ëá™Â∑±ÁöÑÁâπÊÆäÂ∫èÂàóÂè∑+ÂüüÂêçÂç≥ÂèØÔºåÂèØËÉΩÂêéÊúüËøòË¶ÅÂºïÂÖ•ËøáÊúüÊú∫Âà∂„ÄÇÈ´òÂ∫¶ÁÅµÊ¥ªÂèØËá™Áî±ÈÖçÁΩÆÔºåÂ∞ΩÈáèÁÆ°ÊéßÈÉΩÊîæÂú®LinuxÊúçÂä°Á´ØÔºåÂÆ¢Êà∑Windows‰ªÖÊèê‰æõ‰∏Ä‰∫õÊï∞ÊçÆ„ÄÇ

Êû∂ÊûÑÊàëÊÑüËßâÂèØ‰ª•ÂºïÂÖ•Êõ¥Â§ö‰∏ì‰∏öÊó∂Â∞öÊñ∞ÊΩÆÁöÑÁªÑ‰ª∂ÊúçÂä°ÔºåÁîöËá≥Á±ª‰ººtoken / Â∫èÂàóÂè∑ÂèØ‰ª•ÂíåÁî®Êà∑ÂêçÂØÜÁ†ÅÂÖ≥ËÅîÔºåÂèØ‰ª•webÊü•ËØ¢ËøáÊúüÊó•ÊúüÁ≠â„ÄÇÊúâÂøÖË¶ÅÁöÑËØùÂèØ‰ª•ÂºïÂÖ•K3SÔºåÊó∂Â∞öÊñ∞ÊΩÆÊúÄÈáçË¶ÅÔºåÂ∞§ÂÖ∂ÊòØÁî®Êà∑ÂêçÂØÜÁ†Å Â∫èÂàóÂè∑Ëøô‰∏™ÔºåÊúÄÂ•ΩÊúâËá™Âä®‰∏Ä‰∫õÊñπ‰æøÂÆûÁé∞ÁöÑÊñπÊ°àÔºåËøòË¶ÅÊúâËøáÊúüÊó∂Èó¥ÔºåÁî®Êà∑ÂêçÂØÜÁ†Å‰∏çËøáÊúü‰ΩÜÊòØ Â∫èÂàóÂè∑„ÄÅtokenÁ≠âÂèØ‰ª•ÂÆöÊúüËøáÊúü„ÄÇÊàëÂÖ∂ÂÆû‰πü‰∏çÊòØÂàöÈúÄtokenÔºåtokenÂ∫èÂàóÂè∑‰∏ÄÂÆöÊòØÂèØ‰ª•‰∏çÈúÄË¶ÅÁöÑÔºåÂè™Ë¶ÅÊúâÊüêÁßçÈÄöËøáÁî®Êà∑Âêç+ÂØÜÁ†ÅÔºå‰∏îÂèØ‰ª•ËÆæÁΩÆÁßüÊúüÔºåÂπ∂‰∏îËÉΩÂª∂ÈïøÁßüÊúüÁöÑÊâãÊÆµÂç≥ÂèØ„ÄÇËÄå‰∏îÊúÄÂ•ΩËøòÊúâÊñπ‰æøÊü•ËØ¢ÁöÑÊâãÊÆµÔºåÂ¶ÇÊûúÊúâWebUIÊõ¥Â•ΩÔºåÊàë‰πãÂâçÂè™ÊòØÊÉ≥ÂÅ∑ÊáíÁî®Áî®Êà∑Âêç+ÂØÜÁ†Å+Áî®Êà∑ÂêçÂÖ≥ËÅîÁöÑÂ§ö‰∏™ÂèØËøáÊúüÁöÑtokenÊàñËÄÖkey„ÄÇÂ¶ÇÊûúÊúâÂæàÂ§öÊñ∞ÊΩÆÁöÑÁ±ª‰ººÊñπÊ°àËØ∑‰Ω†Â∞ΩÂèØËÉΩÂ§öÁöÑÂêëÊàëÊé®ËçêÔºå‰Ω†ÈÄâÁî®ÁöÑÊúçÂä°ÂíåÊû∂ÊûÑÂèØ‰ª•Â∞ΩÂèØËÉΩÊñ∞„ÄÅÊó∂Â∞ö„ÄÅÁ§æÂå∫ÔºåÂØπ‰∏ì‰∏öÊÄßË¶ÅÊ±Ç‰ΩéÔºåÊúÄÂ•ΩÊúâÁé∞ÊàêÁöÑÁæéËßÇÁöÑÈ°µÈù¢ÔºåË∞¢Ë∞¢„ÄÇÊàë‰∏çÁü•ÈÅìÊÄé‰πàÂÅöËøôÊ†∑ÁöÑÈ°µÈù¢ÊàñËÄÖÊúçÂä°Êû∂ÊûÑ„ÄÇÈô§‰∫ÜÂüüÂêçËØÅ‰π¶ÂíåÁî®Êà∑ÂØÜÁ†Å‰ª•Â§ñÂÖ∂‰ªñÂÆâÂÖ®ÊÄßÂ∞ΩÂèØËÉΩÂéªÊéâÔºå‰øùËØÅÊï¥‰ΩìÊû∂ÊûÑËøûÈÄö„ÄÅÈÉ®ÁΩ≤ÁÆÄ‰æø„ÄÇ

ÊàëÊúüÊúõÊú¨Ë∫´Êû∂ÊûÑÊòØ‰∏ì‰∏öÂåñÁöÑÔºåÂä†ÂÖ•Êõ¥Â§ö‰∫ëÂéüÁîüÁöÑÊÄùÊÉ≥Êû∂ÊûÑÔºåÂèØÊåÅÁª≠ÈÉ®ÁΩ≤ÁÅµÊ¥ªÂºπÊÄßÁ≠âÔºå‰ΩÜÊòØÊû∂ÊûÑ‰∏ãÈÄâÊã©ÁöÑÊúçÂä°ÂèØ‰ª•ÊòØÊñ∞ÊΩÆÁ®≥ÂÆöÊÄß‰∏ì‰∏öÊÄßÊ≤°ÈÇ£‰πàÂº∫ÁöÑ„ÄÇÊØîÂ¶ÇÂèØ‰ª•ÈÖåÊÉÖÂºïÂÖ•argoCDËøòÊúâÂÖ∂‰ªñÁöÑÊúçÂä°„ÄÇ

ËøòÊúâÔºåÂõ†‰∏∫ËøôÊòØ‰∏Ä‰∏™WebÊúçÂä°ÔºåÂ¶ÇÊûú‰Ω†ÊúâÊõ¥Â•ΩÁöÑÂª∫ËÆÆÔºåÊØîÂ¶ÇÂ¶Ç‰ΩïËÆ©Áî®Êà∑ÂæàÁÆÄÂçïÁöÑËÆ§ËØÅËÆøÈóÆÂíåÂàÜ‰∫´ÁªôÊúãÂèãÁΩëÈ°µÔºå‰∏çÈúÄË¶ÅÂÅöÂæàÂ§çÊùÇÁöÑÊìç‰Ωú„ÄÇÁúüÊ≠£Êèê‰æõÊï∞ÊçÆÁöÑÁΩëÁªúÊúçÂä°ÂêéÁ´ØÊàëÊîπ‰∏ç‰∫ÜÔºåhttp://115.120.240.160:19002/?address=115.120.240.160&port=19002&roomId=123456 ÁúãËµ∑Êù•Ëøô‰∏™ÊòØËÉΩÈÅøÂÖçwebUIÁôªÂΩïÁöÑÔºåÊàëËÆ§‰∏∫ÂèØ‰ª•Âà©Áî®ÊüêÁßçÊñπÊ≥ïËá™Âä®ÁîüÊàêÈìæÊé•ÊàñËÄÖÈô§‰∫Üroomid‰ª•Â§ñÂÖ®ÈÉ®ÈÉΩÂèØ‰ª•Áî®‰∏Ä‰∏™ÂüüÂêç‰ª£Êõø„ÄÇ

ÊàëÁõÆÂâçËá™Â∑±ÁöÑÊñπÊ°àÂ¶Ç‰∏ãÔºå‰ºòÂÖà‰øùËØÅÈÄöÔºå‰∏çËÄÉËôëÂÆâÂÖ®ÊÄß„ÄÇ

ÊàëÁé∞Âú®ÈúÄË¶ÅÈÉ®ÁΩ≤Ëá™Âä®ÁªôÁî®Êà∑Â¢ûÂä†24Â∞èÊó∂Ôºå168Â∞èÊó∂Ôºå744Â∞èÊó∂Ôºå2232Â∞èÊó∂Ôºå8784Â∞èÊó∂Âá†ÁßçÔºåÁî®Êà∑ÁôªÈôÜÊñπÂºèÊàëÊ≤°ÊÉ≥Â•ΩÔºåÁî®Êà∑Â¶Ç‰ΩïËÆøÈóÆÁΩëÂùÄÊàë‰πüÊ≤°ÊÉ≥Â•ΩÔºåÂ∞ΩÂèØËÉΩÈôç‰ΩéÁî®Êà∑ÁöÑË¥üÊãÖÔºåË∂äÁÆÄÂçïÊó†ÊÑüË∂äÂ•ΩÔºåÁÑ∂ÂêéÊàëÈ¢ÑÊúü‰πüË¶ÅÂÜô‰∏Ä‰∏™Á®ãÂ∫èÊàñËÄÖÂàùÊúüÂÖàÁî®ËÑöÊú¨Âú®Áî®Êà∑WindowsÊú¨Âú∞ÂêéÂè∞ÂêØÂä®ÂØπÂ∫înginxÂíåfrpcÔºåWindowsÂêØÂä®Ëøô‰∏§‰∏™ËøõÁ®ã‰∏ÄËà¨ÊòØhang‰ΩèÔºåÊàëÊ≤°ÊÉ≥Â•ΩÊ£ÄÊµãËÅîÈÄöÁöÑÊ†áÂøóÔºåËøôÈúÄË¶Å‰Ω†Â∏ÆÊàëÊÉ≥Ëøô‰∫õË¶ÅÂÆûÁé∞ÁöÑÂäüËÉΩÂ¶Ç‰ΩïÂØπÁî®Êà∑Â∞ΩÂèØËÉΩÊó†ÊÑüÁöÑÂÆûÁé∞„ÄÇ

ÊàëÁé∞Âú®ÈúÄË¶Å‰Ω†Â∏ÆÊàëËÆæËÆ°Ëøô‰∏™Â§çÊùÇ‰ΩìÁ≥ªÁöÑÊû∂ÊûÑÔºåËØ∑‰Ω†ÂºÄÂßãËÆæËÆ°ÔºåË∞¢Ë∞¢„ÄÇÁ¶ÅÊ≠¢Êèê‰æõ‰ªª‰ΩïÁöÑÂÖ∑‰ΩìÂëΩ‰ª§„ÄÅ‰ª£Á†ÅÔºåÊàë‰ª¨Áé∞Âú®‰ªÖ‰ΩúÊñπÊ°àÂíåÊû∂ÊûÑËÆæËÆ°ÁöÑËÆ®ËÆ∫„ÄÇ

ÊàëÁõÆÂâçÊúçÂä°Á´ØÈÉ®ÁΩ≤Â§±Ë¥•ÔºåÈúÄË¶Å‰Ω†Â∏ÆÊàëÁªºÂêàÊÄùËÄÉËß£ÂÜ≥ÈóÆÈ¢òÔºö
--> INFO: Deployment Bootstrapper (v23.1) initiated. Full log: /root/personal-cluster/deployment-bootstrap-20250926-001243.log


[1;34m# ============================================================================== #[0m
[1;34m# STEP 0: Ensure Cloudflare wildcard DNS (Timestamp: 2025-09-26T04:12:43+00:00)[0m
[1;34m# ============================================================================== #[0m

--> INFO: Resolving Cloudflare Zone ID for gglohh.top ...
[1;32m‚úÖ SUCCESS:[0m Cloudflare Zone ID acquired: 72377620fc4ce2aef5ba6bfd9c0c4c35
--> INFO: Checking existing DNS record for *.core01.prod.gglohh.top (type A) ...
[1;32m‚úÖ SUCCESS:[0m Wildcard A already correct: *.core01.prod.gglohh.top -> 172.245.187.113 (no action).
--> INFO: Verifying public resolution via 1.1.1.1 ...
[1;32m‚úÖ SUCCESS:[0m Public DNS resolution OK for wildcard subdomain.


[1;34m# ============================================================================== #[0m
[1;34m# STEP 1: System Cleanup (Timestamp: 2025-09-26T04:12:45+00:00)[0m
[1;34m# ============================================================================== #[0m

--> INFO: This step will eradicate all traces of previous K3s and this project's ETCD.
--> INFO: Stopping k3s, docker, and containerd services...
--> INFO: Forcefully removing project's ETCD container and network...
--> INFO: Running K3s uninstaller and cleaning up filesystem...
--> INFO: Reloading systemd and cleaning journals for k3s and docker...
--> INFO: Rotating systemd journal only (no global vacuum; keep unrelated services' logs)...
[1;32m‚úÖ SUCCESS:[0m System cleanup complete.


[1;34m# ============================================================================== #[0m
[1;34m# STEP 2: Deploy and Verify External ETCD (Timestamp: 2025-09-26T04:13:16+00:00)[0m
[1;34m# ============================================================================== #[0m

--> INFO: Preparing ETCD data directory with correct permissions for UID 1001...
--> INFO: Deploying ETCD via Docker...
[1;32m‚úÖ SUCCESS:[0m ETCD container started.
--> INFO: Verifying: ETCD to be healthy (Timeout: 60s)
    ...waiting...
[1;32m‚úÖ SUCCESS:[0m Verified: ETCD to be healthy.


[1;34m# ============================================================================== #[0m
[1;34m# STEP 3: Install and Verify K3S (Timestamp: 2025-09-26T04:13:22+00:00)[0m
[1;34m# ============================================================================== #[0m

--> INFO: Preparing K3s manifest and configuration directories...
--> INFO: Creating Traefik HelmChartConfig with CRD provider and frps (7000/TCP) entryPoint...
[1;32m‚úÖ SUCCESS:[0m K3s customization manifests created.
--> INFO: Installing K3s v1.33.3+k3s1...
[INFO]  Using v1.33.3+k3s1 as release
[INFO]  Downloading hash https://github.com/k3s-io/k3s/releases/download/v1.33.3+k3s1/sha256sum-amd64.txt
[INFO]  Downloading binary https://github.com/k3s-io/k3s/releases/download/v1.33.3+k3s1/k3s
[INFO]  Verifying binary download
[INFO]  Installing k3s to /usr/local/bin/k3s
[INFO]  Skipping installation of SELinux RPM
[INFO]  Skipping /usr/local/bin/kubectl symlink to k3s, already exists
[INFO]  Creating /usr/local/bin/crictl symlink to k3s
[INFO]  Skipping /usr/local/bin/ctr symlink to k3s, command exists in PATH at /usr/bin/ctr
[INFO]  Creating killall script /usr/local/bin/k3s-killall.sh
[INFO]  Creating uninstall script /usr/local/bin/k3s-uninstall.sh
[INFO]  env: Creating environment file /etc/systemd/system/k3s.service.env
[INFO]  systemd: Creating service file /etc/systemd/system/k3s.service
[INFO]  systemd: Enabling k3s unit
Created symlink /etc/systemd/system/multi-user.target.wants/k3s.service ‚Üí /etc/systemd/system/k3s.service.
[INFO]  systemd: Starting k3s
[1;32m‚úÖ SUCCESS:[0m K3s installation script finished.
--> INFO: Setting up kubeconfig for user...
--> INFO: Verifying: K3s node to be Ready (Timeout: 180s)
Error from server (NotFound): nodes "racknerd-f770a87" not found
    ...waiting...
[1;32m‚úÖ SUCCESS:[0m Verified: K3s node to be Ready.
--> INFO: Waiting for HelmChart 'traefik' to appear...
--> INFO: Verifying: HelmChart/traefik exists (Timeout: 240s)
[1;32m‚úÖ SUCCESS:[0m Verified: HelmChart/traefik exists.
[1;32m‚úÖ SUCCESS:[0m HelmChart/traefik detected.
--> INFO: Waiting for job/helm-install-traefik-crd to succeed...
--> INFO: Waiting Job kube-system/helm-install-traefik-crd to succeed...
[1;32m‚úÖ SUCCESS:[0m Job kube-system/helm-install-traefik-crd succeeded.
--> INFO: Waiting for Traefik CRDs to be established in API...
--> INFO: Verifying: CRD ingressroutes.traefik.io present (Timeout: 180s)
[1;32m‚úÖ SUCCESS:[0m Verified: CRD ingressroutes.traefik.io present.
--> INFO: Verifying: CRD ingressroutetcps.traefik.io present (Timeout: 180s)
[1;32m‚úÖ SUCCESS:[0m Verified: CRD ingressroutetcps.traefik.io present.
--> INFO: Verifying: CRD ingressrouteudps.traefik.io present (Timeout: 180s)
[1;32m‚úÖ SUCCESS:[0m Verified: CRD ingressrouteudps.traefik.io present.
--> INFO: Verifying: CRD middlewares.traefik.io present (Timeout: 180s)
[1;32m‚úÖ SUCCESS:[0m Verified: CRD middlewares.traefik.io present.
--> INFO: Verifying: CRD traefikservices.traefik.io present (Timeout: 180s)
[1;32m‚úÖ SUCCESS:[0m Verified: CRD traefikservices.traefik.io present.
--> INFO: Verifying: CRD tlsoptions.traefik.io present (Timeout: 180s)
[1;32m‚úÖ SUCCESS:[0m Verified: CRD tlsoptions.traefik.io present.
--> INFO: Verifying: CRD serverstransports.traefik.io present (Timeout: 180s)
[1;32m‚úÖ SUCCESS:[0m Verified: CRD serverstransports.traefik.io present.
[1;32m‚úÖ SUCCESS:[0m All Traefik CRDs are present.
--> INFO: Waiting for job/helm-install-traefik to succeed...
--> INFO: Waiting Job kube-system/helm-install-traefik to succeed...
[1;32m‚úÖ SUCCESS:[0m Job kube-system/helm-install-traefik succeeded.
--> INFO: Waiting for Traefik Deployment to be created...
--> INFO: Verifying: Deployment/traefik exists (Timeout: 240s)
[1;32m‚úÖ SUCCESS:[0m Verified: Deployment/traefik exists.
--> INFO: Waiting for Traefik Deployment rollout...
--> INFO: Verifying: Traefik Deployment rollout (Timeout: 480s)
[1;32m‚úÖ SUCCESS:[0m Verified: Traefik Deployment rollout.
[1;32m‚úÖ SUCCESS:[0m Traefik Deployment is Ready.
--> INFO: Checking Service/traefik exposes required ports (80, 443, 7000)...
--> INFO: Verifying: Service/traefik to expose 80,443,7000 (Timeout: 300s)
[1;32m‚úÖ SUCCESS:[0m Verified: Service/traefik to expose 80,443,7000.
[1;32m‚úÖ SUCCESS:[0m Traefik Service exposes 80/443/7000.


[1;34m# ============================================================================== #[0m
[1;34m# STEP 4: Bootstrap GitOps Engine (Argo CD) (Timestamp: 2025-09-26T04:14:41+00:00)[0m
[1;34m# ============================================================================== #[0m

--> INFO: Bootstrapping Argo CD via Helm...
--> INFO: This initial install will create CRDs and components with static credentials.
Release "argocd" does not exist. Installing it now.
NAME: argocd
LAST DEPLOYED: Fri Sep 26 00:14:43 2025
NAMESPACE: argocd
STATUS: deployed
REVISION: 1
TEST SUITE: None
NOTES:
In order to access the server UI you have the following options:

1. kubectl port-forward service/argocd-server -n argocd 8080:443

    and then open the browser on http://localhost:8080 and accept the certificate

2. enable ingress in the values file `server.ingress.enabled` and either
      - Add the annotation for ssl passthrough: https://argo-cd.readthedocs.io/en/stable/operator-manual/ingress/#option-1-ssl-passthrough
      - Set the `configs.params."server.insecure"` in the values file and terminate SSL at your ingress: https://argo-cd.readthedocs.io/en/stable/operator-manual/ingress/#option-2-multiple-ingress-objects-and-hosts


After reaching the UI the first time you can login with username: admin and the random password generated during the installation. You can find the password by running:

kubectl -n argocd get secret argocd-initial-admin-secret -o jsonpath="{.data.password}" | base64 -d

(You should delete the initial secret afterwards as suggested by the Getting Started Guide: https://argo-cd.readthedocs.io/en/stable/getting_started/#4-login-using-the-cli)
[1;32m‚úÖ SUCCESS:[0m Argo CD components and CRDs installed via Helm with static password.
--> INFO: Applying Argo CD application manifests to enable GitOps self-management...
Warning: metadata.finalizers: "resources-finalizer.argocd.argoproj.io": prefer a domain-qualified finalizer name including a path (/) to avoid accidental conflicts with other finalizer writers
application.argoproj.io/argocd created
--> INFO: Waiting for Argo CD to sync its own application resource...
--> INFO: Verifying: Argo CD to become Healthy and self-managed (Timeout: 300s)
    ...waiting...
[1;32m‚úÖ SUCCESS:[0m Verified: Argo CD to become Healthy and self-managed.
[1;32m‚úÖ SUCCESS:[0m Argo CD has been bootstrapped and is now self-managing via GitOps.


[1;34m# ============================================================================== #[0m
[1;34m# STEP 5: Deploy Core Applications via GitOps (Timestamp: 2025-09-26T04:15:57+00:00)[0m
[1;34m# ============================================================================== #[0m

--> INFO: Applying cert-manager Application (only)...
application.argoproj.io/cert-manager created
--> INFO: Checking Kubernetes apiserver readiness (/readyz) with timeout 300s...
[1;32m‚úÖ SUCCESS:[0m Kubernetes apiserver reports Ready.
--> INFO: Waiting for cert-manager Deployments to become Available...
    ...waiting for cert-manager deployments to appear
    ...waiting for cert-manager deployments to appear
    ...waiting for cert-manager deployments to appear
    ...waiting for cert-manager deployments to appear
    ...waiting for cert-manager deployments to appear
Waiting for deployment "cert-manager" rollout to finish: 0 of 1 updated replicas are available...
deployment "cert-manager" successfully rolled out
deployment "cert-manager-webhook" successfully rolled out
[1;32m‚úÖ SUCCESS:[0m cert-manager core Deployments are Available.
--> INFO: Checking Kubernetes apiserver readiness (/readyz) with timeout 300s...
[1;32m‚úÖ SUCCESS:[0m Kubernetes apiserver reports Ready.
--> INFO: Waiting for Cert-Manager application to become Healthy in Argo CD...
--> INFO: Verifying: Cert-Manager Argo CD App to be Healthy (Timeout: 100s)
    ...waiting...
    ...waiting...
    ...waiting...
    ...waiting...
[1;32m‚úÖ SUCCESS:[0m Verified: Cert-Manager Argo CD App to be Healthy.
[1;32m‚úÖ SUCCESS:[0m Cert-Manager application is Healthy in Argo CD.
--> INFO: Applying remaining Applications (excluding n8n)...
application.argoproj.io/frps created
application.argoproj.io/core-manifests created
application.argoproj.io/argocd-ingress created
application.argoproj.io/provisioner created
application.argoproj.io/authentik-ingress-static created
--> INFO: Waiting for core-manifests application to become Healthy...
--> INFO: Verifying: core-manifests Argo CD App to be Healthy (Timeout: 600s)
    ...waiting...
[1;32m‚úÖ SUCCESS:[0m Verified: core-manifests Argo CD App to be Healthy.
--> INFO: Waiting for argocd-ingress application to become Healthy...
--> INFO: Verifying: argocd-ingress Argo CD App to be Healthy (Timeout: 600s)
    ...waiting...
    ...waiting...
    ...waiting...
    ...waiting...
    ...waiting...
    ...waiting...
    ...waiting...
    ...waiting...
    ...waiting...
[1;32m‚úÖ SUCCESS:[0m Verified: argocd-ingress Argo CD App to be Healthy.
--> INFO: Waiting for provisioner application to become Healthy...
--> INFO: Verifying: provisioner Argo CD App to be Healthy (Timeout: 600s)
    ...waiting...
[1;32m‚úÖ SUCCESS:[0m Verified: provisioner Argo CD App to be Healthy.
[1;32m‚úÖ SUCCESS:[0m provisioner application is Healthy.
--> INFO: Waiting for authentik-ingress-static application to become Healthy...
--> INFO: Verifying: authentik-ingress-static Argo CD App to be Healthy (Timeout: 600s)
    ...waiting...
[1;32m‚úÖ SUCCESS:[0m Verified: authentik-ingress-static Argo CD App to be Healthy.
[1;32m‚úÖ SUCCESS:[0m Remaining applications submitted and Healthy.


[1;34m# ============================================================================== #[0m
[1;34m# STEP 6: Final End-to-End Verification (Timestamp: 2025-09-26T04:19:40+00:00)[0m
[1;34m# ============================================================================== #[0m

--> INFO: Checking Kubernetes apiserver readiness (/readyz) with timeout 180s...
[1;32m‚úÖ SUCCESS:[0m Kubernetes apiserver reports Ready.
--> INFO: Verifying ClusterIssuer 'cloudflare-staging' is ready...
--> INFO: Verifying: ClusterIssuer to be Ready (Timeout: 120s)
[1;32m‚úÖ SUCCESS:[0m Verified: ClusterIssuer to be Ready.
--> INFO: Verifying ArgoCD IngressRoute certificate has been issued...
--> INFO: Verifying: Certificate to be Ready (Timeout: 300s)
[1;32m‚úÖ SUCCESS:[0m Verified: Certificate to be Ready.
--> INFO: Performing final reachability check on ArgoCD URL: https://argocd.core01.prod.gglohh.top
--> INFO: Verifying: ArgoCD UI to be reachable (HTTP 200 OK) (Timeout: 120s)
[1;32m‚úÖ SUCCESS:[0m Verified: ArgoCD UI to be reachable (HTTP 200 OK).
--> INFO: Verifying Provisioner portal Certificate has been issued...
--> INFO: Verifying: Portal Certificate to be Ready (Timeout: 300s)
[1;32m‚úÖ SUCCESS:[0m Verified: Portal Certificate to be Ready.
--> INFO: Waiting for provisioner-gateway Deployment rollout...
--> INFO: Verifying: provisioner-gateway rollout to complete (Timeout: 240s)
[1;32m‚úÖ SUCCESS:[0m Verified: provisioner-gateway rollout to complete.
--> INFO: Waiting for Service/provisioner-gateway endpoints to be populated...
--> INFO: Verifying: provisioner-gateway Endpoints to be Ready (Timeout: 240s)
Warning: v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
[1;32m‚úÖ SUCCESS:[0m Verified: provisioner-gateway Endpoints to be Ready.
--> INFO: Verifying TLS Secret 'portal-tls-staging' exists for Traefik...
--> INFO: Verifying: Secret portal-tls-staging available (Timeout: 180s)
[1;32m‚úÖ SUCCESS:[0m Verified: Secret portal-tls-staging available.
--> INFO: Allowing Traefik to resync TLS assets...
--> INFO: Performing reachability check on Portal URL: https://portal.core01.prod.gglohh.top
--> INFO: Verifying: Provisioner portal to be reachable (HTTP 200/30x) (Timeout: 180s)
    ...waiting...
    ...waiting...
    ...waiting...
    ...waiting...
    ...waiting...
    ...waiting...
    ...waiting...
    ...waiting...
    ...waiting...
    ...waiting...
    ...waiting...
    ...waiting...
    ...waiting...
    ...waiting...
    ...waiting...
    ...waiting...
    ...waiting...
    ...waiting...
[1;33m‚ö†Ô∏è  WARN: Condition 'Provisioner portal to be reachable (HTTP 200/30x)' was NOT met within the timeout period.[0m
[90m2025-09-26T04:14:39Z[0m [32mINF[0m [1mTraefik version 3.3.6 built on 2025-04-18T09:18:47Z[0m [36mversion=[0m3.3.6
[90m2025-09-26T04:14:39Z[0m [32mINF[0m [1mStats collection is enabled.[0m
[90m2025-09-26T04:14:39Z[0m [32mINF[0m [1mMany thanks for contributing to Traefik's improvement by allowing us to receive anonymous information from your configuration.[0m
[90m2025-09-26T04:14:39Z[0m [32mINF[0m [1mHelp us improve Traefik by leaving this feature on :)[0m
[90m2025-09-26T04:14:39Z[0m [32mINF[0m [1mMore details on: https://doc.traefik.io/traefik/contributing/data-collection/[0m
[90m2025-09-26T04:14:39Z[0m [32mINF[0m [1mStarting provider aggregator *aggregator.ProviderAggregator[0m
[90m2025-09-26T04:14:39Z[0m [32mINF[0m [1mStarting provider *acme.ChallengeTLSALPN[0m
[90m2025-09-26T04:14:39Z[0m [32mINF[0m [1mStarting provider *crd.Provider[0m
[90m2025-09-26T04:14:39Z[0m [32mINF[0m [1mStarting provider *traefik.Provider[0m
[90m2025-09-26T04:14:39Z[0m [32mINF[0m [1mlabel selector is: ""[0m [36mproviderName=[0mkubernetescrd
[90m2025-09-26T04:14:39Z[0m [32mINF[0m [1mCreating in-cluster Provider client[0m [36mproviderName=[0mkubernetescrd
[90m2025-09-26T04:14:39Z[0m [32mINF[0m [1mStarting provider *ingress.Provider[0m
[90m2025-09-26T04:14:39Z[0m [32mINF[0m [1mingress label selector is: ""[0m [36mproviderName=[0mkubernetes
[90m2025-09-26T04:14:39Z[0m [32mINF[0m [1mCreating in-cluster Provider client[0m [36mproviderName=[0mkubernetes
[90m2025-09-26T04:17:33Z[0m [31mERR[0m [1mError configuring TLS[0m [36merror=[0m[31m[1m"secret argocd/argocd-server-tls-staging does not exist"[0m[0m [36mingress=[0margocd-server-https [36mnamespace=[0margocd [36mproviderName=[0mkubernetescrd
[90m2025-09-26T04:17:35Z[0m [31mERR[0m [1mError configuring TLS[0m [36merror=[0m[31m[1m"secret argocd/argocd-server-tls-staging does not exist"[0m[0m [36mingress=[0margocd-server-https [36mnamespace=[0margocd [36mproviderName=[0mkubernetescrd
[90m2025-09-26T04:17:35Z[0m [31mERR[0m [1mError configuring TLS[0m [36merror=[0m[31m[1m"secret argocd/argocd-server-tls-staging does not exist"[0m[0m [36mingress=[0margocd-server-https [36mnamespace=[0margocd [36mproviderName=[0mkubernetescrd
[90m2025-09-26T04:17:35Z[0m [31mERR[0m [1mError configuring TLS[0m [36merror=[0m[31m[1m"secret argocd/argocd-server-tls-staging does not exist"[0m[0m [36mingress=[0margocd-server-https [36mnamespace=[0margocd [36mproviderName=[0mkubernetescrd
[90m2025-09-26T04:17:35Z[0m [31mERR[0m [1mError configuring TLS[0m [36merror=[0m[31m[1m"secret authentik/authentik-tls-staging does not exist"[0m[0m [36mingress=[0mauthentik-https [36mnamespace=[0mauthentik [36mproviderName=[0mkubernetescrd
[90m2025-09-26T04:17:35Z[0m [31mERR[0m [36merror=[0m[31m[1m"kubernetes service not found: authentik/authentik-server"[0m[0m [36mingress=[0mauthentik-https [36mnamespace=[0mauthentik [36mproviderName=[0mkubernetescrd
[90m2025-09-26T04:17:36Z[0m [31mERR[0m [1mError configuring TLS[0m [36merror=[0m[31m[1m"secret argocd/argocd-server-tls-staging does not exist"[0m[0m [36mingress=[0margocd-server-https [36mnamespace=[0margocd [36mproviderName=[0mkubernetescrd
[90m2025-09-26T04:17:36Z[0m [31mERR[0m [1mError configuring TLS[0m [36merror=[0m[31m[1m"secret authentik/authentik-tls-staging does not exist"[0m[0m [36mingress=[0mauthentik-https [36mnamespace=[0mauthentik [36mproviderName=[0mkubernetescrd
[90m2025-09-26T04:17:36Z[0m [31mERR[0m [36merror=[0m[31m[1m"kubernetes service not found: authentik/authentik-server"[0m[0m [36mingress=[0mauthentik-https [36mnamespace=[0mauthentik [36mproviderName=[0mkubernetescrd
[90m2025-09-26T04:17:40Z[0m [31mERR[0m [1mError configuring TLS[0m [36merror=[0m[31m[1m"secret argocd/argocd-server-tls-staging does not exist"[0m[0m [36mingress=[0margocd-server-https [36mnamespace=[0margocd [36mproviderName=[0mkubernetescrd
[90m2025-09-26T04:17:40Z[0m [31mERR[0m [1mError configuring TLS[0m [36merror=[0m[31m[1m"secret authentik/authentik-tls-staging does not exist"[0m[0m [36mingress=[0mauthentik-https [36mnamespace=[0mauthentik [36mproviderName=[0mkubernetescrd
[90m2025-09-26T04:17:40Z[0m [31mERR[0m [36merror=[0m[31m[1m"kubernetes service not found: authentik/authentik-server"[0m[0m [36mingress=[0mauthentik-https [36mnamespace=[0mauthentik [36mproviderName=[0mkubernetescrd
[90m2025-09-26T04:17:41Z[0m [31mERR[0m [1mError configuring TLS[0m [36merror=[0m[31m[1m"secret argocd/argocd-server-tls-staging does not exist"[0m[0m [36mingress=[0margocd-server-https [36mnamespace=[0margocd [36mproviderName=[0mkubernetescrd
[90m2025-09-26T04:17:41Z[0m [31mERR[0m [1mError configuring TLS[0m [36merror=[0m[31m[1m"secret authentik/authentik-tls-staging does not exist"[0m[0m [36mingress=[0mauthentik-https [36mnamespace=[0mauthentik [36mproviderName=[0mkubernetescrd
[90m2025-09-26T04:17:41Z[0m [31mERR[0m [36merror=[0m[31m[1m"kubernetes service not found: authentik/authentik-server"[0m[0m [36mingress=[0mauthentik-https [36mnamespace=[0mauthentik [36mproviderName=[0mkubernetescrd
[90m2025-09-26T04:17:41Z[0m [31mERR[0m [1mError configuring TLS[0m [36merror=[0m[31m[1m"secret argocd/argocd-server-tls-staging does not exist"[0m[0m [36mingress=[0margocd-server-https [36mnamespace=[0margocd [36mproviderName=[0mkubernetescrd
[90m2025-09-26T04:17:41Z[0m [31mERR[0m [1mError configuring TLS[0m [36merror=[0m[31m[1m"secret authentik/authentik-tls-staging does not exist"[0m[0m [36mingress=[0mauthentik-https [36mnamespace=[0mauthentik [36mproviderName=[0mkubernetescrd
[90m2025-09-26T04:17:41Z[0m [31mERR[0m [36merror=[0m[31m[1m"kubernetes service not found: authentik/authentik-server"[0m[0m [36mingress=[0mauthentik-https [36mnamespace=[0mauthentik [36mproviderName=[0mkubernetescrd
[90m2025-09-26T04:17:49Z[0m [31mERR[0m [1mError configuring TLS[0m [36merror=[0m[31m[1m"secret argocd/argocd-server-tls-staging does not exist"[0m[0m [36mingress=[0margocd-server-https [36mnamespace=[0margocd [36mproviderName=[0mkubernetescrd
[90m2025-09-26T04:17:49Z[0m [31mERR[0m [1mError configuring TLS[0m [36merror=[0m[31m[1m"secret authentik/authentik-tls-staging does not exist"[0m[0m [36mingress=[0mauthentik-https [36mnamespace=[0mauthentik [36mproviderName=[0mkubernetescrd
[90m2025-09-26T04:17:49Z[0m [31mERR[0m [36merror=[0m[31m[1m"kubernetes service not found: authentik/authentik-server"[0m[0m [36mingress=[0mauthentik-https [36mnamespace=[0mauthentik [36mproviderName=[0mkubernetescrd
[90m2025-09-26T04:17:50Z[0m [31mERR[0m [1mError configuring TLS[0m [36merror=[0m[31m[1m"secret argocd/argocd-server-tls-staging does not exist"[0m[0m [36mingress=[0margocd-server-https [36mnamespace=[0margocd [36mproviderName=[0mkubernetescrd
[90m2025-09-26T04:17:50Z[0m [31mERR[0m [1mError configuring TLS[0m [36merror=[0m[31m[1m"secret authentik/authentik-tls-staging does not exist"[0m[0m [36mingress=[0mauthentik-https [36mnamespace=[0mauthentik [36mproviderName=[0mkubernetescrd
[90m2025-09-26T04:17:50Z[0m [31mERR[0m [36merror=[0m[31m[1m"kubernetes service not found: authentik/authentik-server"[0m[0m [36mingress=[0mauthentik-https [36mnamespace=[0mauthentik [36mproviderName=[0mkubernetescrd
[90m2025-09-26T04:17:51Z[0m [31mERR[0m [1mError configuring TLS[0m [36merror=[0m[31m[1m"secret argocd/argocd-server-tls-staging does not exist"[0m[0m [36mingress=[0margocd-server-https [36mnamespace=[0margocd [36mproviderName=[0mkubernetescrd
[90m2025-09-26T04:17:51Z[0m [31mERR[0m [1mError configuring TLS[0m [36merror=[0m[31m[1m"secret authentik/authentik-tls-staging does not exist"[0m[0m [36mingress=[0mauthentik-https [36mnamespace=[0mauthentik [36mproviderName=[0mkubernetescrd
[90m2025-09-26T04:17:51Z[0m [31mERR[0m [36merror=[0m[31m[1m"kubernetes service not found: authentik/authentik-server"[0m[0m [36mingress=[0mauthentik-https [36mnamespace=[0mauthentik [36mproviderName=[0mkubernetescrd
[90m2025-09-26T04:17:52Z[0m [31mERR[0m [1mError configuring TLS[0m [36merror=[0m[31m[1m"secret argocd/argocd-server-tls-staging does not exist"[0m[0m [36mingress=[0margocd-server-https [36mnamespace=[0margocd [36mproviderName=[0mkubernetescrd
[90m2025-09-26T04:17:52Z[0m [31mERR[0m [1mError configuring TLS[0m [36merror=[0m[31m[1m"secret authentik/authentik-tls-staging does not exist"[0m[0m [36mingress=[0mauthentik-https [36mnamespace=[0mauthentik [36mproviderName=[0mkubernetescrd
[90m2025-09-26T04:17:52Z[0m [31mERR[0m [36merror=[0m[31m[1m"kubernetes service not found: authentik/authentik-server"[0m[0m [36mingress=[0mauthentik-https [36mnamespace=[0mauthentik [36mproviderName=[0mkubernetescrd
[90m2025-09-26T04:17:56Z[0m [31mERR[0m [1mError configuring TLS[0m [36merror=[0m[31m[1m"secret argocd/argocd-server-tls-staging does not exist"[0m[0m [36mingress=[0margocd-server-https [36mnamespace=[0margocd [36mproviderName=[0mkubernetescrd
[90m2025-09-26T04:17:56Z[0m [31mERR[0m [1mError configuring TLS[0m [36merror=[0m[31m[1m"secret authentik/authentik-tls-staging does not exist"[0m[0m [36mingress=[0mauthentik-https [36mnamespace=[0mauthentik [36mproviderName=[0mkubernetescrd
[90m2025-09-26T04:17:56Z[0m [31mERR[0m [36merror=[0m[31m[1m"kubernetes service not found: authentik/authentik-server"[0m[0m [36mingress=[0mauthentik-https [36mnamespace=[0mauthentik [36mproviderName=[0mkubernetescrd
[90m2025-09-26T04:18:08Z[0m [31mERR[0m [1mError configuring TLS[0m [36merror=[0m[31m[1m"secret argocd/argocd-server-tls-staging does not exist"[0m[0m [36mingress=[0margocd-server-https [36mnamespace=[0margocd [36mproviderName=[0mkubernetescrd
[90m2025-09-26T04:18:08Z[0m [31mERR[0m [1mError configuring TLS[0m [36merror=[0m[31m[1m"secret authentik/authentik-tls-staging does not exist"[0m[0m [36mingress=[0mauthentik-https [36mnamespace=[0mauthentik [36mproviderName=[0mkubernetescrd
[90m2025-09-26T04:18:08Z[0m [31mERR[0m [36merror=[0m[31m[1m"kubernetes service not found: authentik/authentik-server"[0m[0m [36mingress=[0mauthentik-https [36mnamespace=[0mauthentik [36mproviderName=[0mkubernetescrd
[90m2025-09-26T04:18:08Z[0m [31mERR[0m [1mError configuring TLS[0m [36merror=[0m[31m[1m"secret argocd/argocd-server-tls-staging does not exist"[0m[0m [36mingress=[0margocd-server-https [36mnamespace=[0margocd [36mproviderName=[0mkubernetescrd
[90m2025-09-26T04:18:08Z[0m [31mERR[0m [1mError configuring TLS[0m [36merror=[0m[31m[1m"secret authentik/authentik-tls-staging does not exist"[0m[0m [36mingress=[0mauthentik-https [36mnamespace=[0mauthentik [36mproviderName=[0mkubernetescrd
[90m2025-09-26T04:18:08Z[0m [31mERR[0m [36merror=[0m[31m[1m"kubernetes service not found: authentik/authentik-server"[0m[0m [36mingress=[0mauthentik-https [36mnamespace=[0mauthentik [36mproviderName=[0mkubernetescrd
[90m2025-09-26T04:18:09Z[0m [31mERR[0m [1mError configuring TLS[0m [36merror=[0m[31m[1m"secret argocd/argocd-server-tls-staging does not exist"[0m[0m [36mingress=[0margocd-server-https [36mnamespace=[0margocd [36mproviderName=[0mkubernetescrd
[90m2025-09-26T04:18:09Z[0m [31mERR[0m [1mError configuring TLS[0m [36merror=[0m[31m[1m"secret authentik/authentik-tls-staging does not exist"[0m[0m [36mingress=[0mauthentik-https [36mnamespace=[0mauthentik [36mproviderName=[0mkubernetescrd
[90m2025-09-26T04:18:09Z[0m [31mERR[0m [36merror=[0m[31m[1m"kubernetes service not found: authentik/authentik-server"[0m[0m [36mingress=[0mauthentik-https [36mnamespace=[0mauthentik [36mproviderName=[0mkubernetescrd
[90m2025-09-26T04:18:26Z[0m [31mERR[0m [1mError configuring TLS[0m [36merror=[0m[31m[1m"secret argocd/argocd-server-tls-staging does not exist"[0m[0m [36mingress=[0margocd-server-https [36mnamespace=[0margocd [36mproviderName=[0mkubernetescrd
[90m2025-09-26T04:18:26Z[0m [31mERR[0m [1mError configuring TLS[0m [36merror=[0m[31m[1m"secret authentik/authentik-tls-staging does not exist"[0m[0m [36mingress=[0mauthentik-https [36mnamespace=[0mauthentik [36mproviderName=[0mkubernetescrd
[90m2025-09-26T04:18:26Z[0m [31mERR[0m [36merror=[0m[31m[1m"kubernetes service not found: authentik/authentik-server"[0m[0m [36mingress=[0mauthentik-https [36mnamespace=[0mauthentik [36mproviderName=[0mkubernetescrd
[90m2025-09-26T04:19:11Z[0m [31mERR[0m [1mError configuring TLS[0m [36merror=[0m[31m[1m"secret authentik/authentik-tls-staging does not exist"[0m[0m [36mingress=[0mauthentik-https [36mnamespace=[0mauthentik [36mproviderName=[0mkubernetescrd
[90m2025-09-26T04:19:11Z[0m [31mERR[0m [36merror=[0m[31m[1m"kubernetes service not found: authentik/authentik-server"[0m[0m [36mingress=[0mauthentik-https [36mnamespace=[0mauthentik [36mproviderName=[0mkubernetescrd
[90m2025-09-26T04:19:11Z[0m [31mERR[0m [1mError configuring TLS[0m [36merror=[0m[31m[1m"secret authentik/authentik-tls-staging does not exist"[0m[0m [36mingress=[0mauthentik-https [36mnamespace=[0mauthentik [36mproviderName=[0mkubernetescrd
[90m2025-09-26T04:19:11Z[0m [31mERR[0m [36merror=[0m[31m[1m"kubernetes service not found: authentik/authentik-server"[0m[0m [36mingress=[0mauthentik-https [36mnamespace=[0mauthentik [36mproviderName=[0mkubernetescrd
[90m2025-09-26T04:19:24Z[0m [31mERR[0m [1mError configuring TLS[0m [36merror=[0m[31m[1m"secret authentik/authentik-tls-staging does not exist"[0m[0m [36mingress=[0mauthentik-https [36mnamespace=[0mauthentik [36mproviderName=[0mkubernetescrd
[90m2025-09-26T04:19:24Z[0m [31mERR[0m [36merror=[0m[31m[1m"kubernetes service not found: authentik/authentik-server"[0m[0m [36mingress=[0mauthentik-https [36mnamespace=[0mauthentik [36mproviderName=[0mkubernetescrd
[90m2025-09-26T04:19:24Z[0m [31mERR[0m [1mError configuring TLS[0m [36merror=[0m[31m[1m"secret authentik/authentik-tls-staging does not exist"[0m[0m [36mingress=[0mauthentik-https [36mnamespace=[0mauthentik [36mproviderName=[0mkubernetescrd
[90m2025-09-26T04:19:24Z[0m [31mERR[0m [36merror=[0m[31m[1m"kubernetes service not found: authentik/authentik-server"[0m[0m [36mingress=[0mauthentik-https [36mnamespace=[0mauthentik [36mproviderName=[0mkubernetescrd
[90m2025-09-26T04:19:25Z[0m [31mERR[0m [1mError configuring TLS[0m [36merror=[0m[31m[1m"secret authentik/authentik-tls-staging does not exist"[0m[0m [36mingress=[0mauthentik-https [36mnamespace=[0mauthentik [36mproviderName=[0mkubernetescrd
[90m2025-09-26T04:19:25Z[0m [31mERR[0m [36merror=[0m[31m[1m"kubernetes service not found: authentik/authentik-server"[0m[0m [36mingress=[0mauthentik-https [36mnamespace=[0mauthentik [36mproviderName=[0mkubernetescrd
[90m2025-09-26T04:19:26Z[0m [31mERR[0m [1mError configuring TLS[0m [36merror=[0m[31m[1m"secret authentik/authentik-tls-staging does not exist"[0m[0m [36mingress=[0mauthentik-https [36mnamespace=[0mauthentik [36mproviderName=[0mkubernetescrd
[90m2025-09-26T04:19:26Z[0m [31mERR[0m [36merror=[0m[31m[1m"kubernetes service not found: authentik/authentik-server"[0m[0m [36mingress=[0mauthentik-https [36mnamespace=[0mauthentik [36mproviderName=[0mkubernetescrd
[90m2025-09-26T04:19:26Z[0m [31mERR[0m [1mError configuring TLS[0m [36merror=[0m[31m[1m"secret authentik/authentik-tls-staging does not exist"[0m[0m [36mingress=[0mauthentik-https [36mnamespace=[0mauthentik [36mproviderName=[0mkubernetescrd
[90m2025-09-26T04:19:26Z[0m [31mERR[0m [36merror=[0m[31m[1m"kubernetes service not found: authentik/authentik-server"[0m[0m [36mingress=[0mauthentik-https [36mnamespace=[0mauthentik [36mproviderName=[0mkubernetescrd
[90m2025-09-26T04:19:34Z[0m [31mERR[0m [36merror=[0m[31m[1m"kubernetes service not found: authentik/authentik-server"[0m[0m [36mingress=[0mauthentik-https [36mnamespace=[0mauthentik [36mproviderName=[0mkubernetescrd
[90m2025-09-26T04:19:34Z[0m [31mERR[0m [36merror=[0m[31m[1m"kubernetes service not found: authentik/authentik-server"[0m[0m [36mingress=[0mauthentik-https [36mnamespace=[0mauthentik [36mproviderName=[0mkubernetescrd
Listening on port 80.
{"name":"echo-server","hostname":"provisioner-gateway-fcbff6984-wc795","pid":1,"level":30,"host":{"hostname":"10.42.0.22","ip":"::ffff:10.42.0.1","ips":[]},"http":{"method":"GET","baseUrl":"","originalUrl":"/","protocol":"http"},"request":{"params":{},"query":{},"cookies":{},"body":{},"headers":{"host":"10.42.0.22:80","user-agent":"kube-probe/1.33","accept":"*/*","connection":"close"}},"environment":{"PATH":"/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin","HOSTNAME":"provisioner-gateway-fcbff6984-wc795","NODE_VERSION":"20.11.0","YARN_VERSION":"1.22.19","KUBERNETES_SERVICE_PORT":"443","PROVISIONER_GATEWAY_PORT_80_TCP_PORT":"80","PROVISIONER_GATEWAY_PORT_80_TCP_ADDR":"10.43.127.120","KUBERNETES_SERVICE_PORT_HTTPS":"443","KUBERNETES_PORT":"tcp://10.43.0.1:443","KUBERNETES_PORT_443_TCP":"tcp://10.43.0.1:443","PROVISIONER_GATEWAY_PORT_80_TCP_PROTO":"tcp","PROVISIONER_GATEWAY_SERVICE_HOST":"10.43.127.120","PROVISIONER_GATEWAY_PORT":"tcp://10.43.127.120:80","KUBERNETES_SERVICE_HOST":"10.43.0.1","KUBERNETES_PORT_443_TCP_PROTO":"tcp","KUBERNETES_PORT_443_TCP_PORT":"443","KUBERNETES_PORT_443_TCP_ADDR":"10.43.0.1","PROVISIONER_GATEWAY_SERVICE_PORT":"80","PROVISIONER_GATEWAY_SERVICE_PORT_HTTP":"80","PROVISIONER_GATEWAY_PORT_80_TCP":"tcp://10.43.127.120:80","HOME":"/root"},"msg":"Fri, 26 Sep 2025 04:18:08 GMT | [GET] - http://10.42.0.22:80/","time":"2025-09-26T04:18:08.698Z","v":0}
{"name":"echo-server","hostname":"provisioner-gateway-fcbff6984-wc795","pid":1,"level":30,"host":{"hostname":"10.42.0.22","ip":"::ffff:10.42.0.1","ips":[]},"http":{"method":"GET","baseUrl":"","originalUrl":"/","protocol":"http"},"request":{"params":{},"query":{},"cookies":{},"body":{},"headers":{"host":"10.42.0.22:80","user-agent":"kube-probe/1.33","accept":"*/*","connection":"close"}},"environment":{"PATH":"/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin","HOSTNAME":"provisioner-gateway-fcbff6984-wc795","NODE_VERSION":"20.11.0","YARN_VERSION":"1.22.19","KUBERNETES_SERVICE_PORT":"443","PROVISIONER_GATEWAY_PORT_80_TCP_PORT":"80","PROVISIONER_GATEWAY_PORT_80_TCP_ADDR":"10.43.127.120","KUBERNETES_SERVICE_PORT_HTTPS":"443","KUBERNETES_PORT":"tcp://10.43.0.1:443","KUBERNETES_PORT_443_TCP":"tcp://10.43.0.1:443","PROVISIONER_GATEWAY_PORT_80_TCP_PROTO":"tcp","PROVISIONER_GATEWAY_SERVICE_HOST":"10.43.127.120","PROVISIONER_GATEWAY_PORT":"tcp://10.43.127.120:80","KUBERNETES_SERVICE_HOST":"10.43.0.1","KUBERNETES_PORT_443_TCP_PROTO":"tcp","KUBERNETES_PORT_443_TCP_PORT":"443","KUBERNETES_PORT_443_TCP_ADDR":"10.43.0.1","PROVISIONER_GATEWAY_SERVICE_PORT":"80","PROVISIONER_GATEWAY_SERVICE_PORT_HTTP":"80","PROVISIONER_GATEWAY_PORT_80_TCP":"tcp://10.43.127.120:80","HOME":"/root"},"msg":"Fri, 26 Sep 2025 04:18:13 GMT | [GET] - http://10.42.0.22:80/","time":"2025-09-26T04:18:13.664Z","v":0}
{"name":"echo-server","hostname":"provisioner-gateway-fcbff6984-wc795","pid":1,"level":30,"host":{"hostname":"10.42.0.22","ip":"::ffff:10.42.0.1","ips":[]},"http":{"method":"GET","baseUrl":"","originalUrl":"/","protocol":"http"},"request":{"params":{},"query":{},"cookies":{},"body":{},"headers":{"host":"10.42.0.22:80","user-agent":"kube-probe/1.33","accept":"*/*","connection":"close"}},"environment":{"PATH":"/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin","HOSTNAME":"provisioner-gateway-fcbff6984-wc795","NODE_VERSION":"20.11.0","YARN_VERSION":"1.22.19","KUBERNETES_SERVICE_PORT":"443","PROVISIONER_GATEWAY_PORT_80_TCP_PORT":"80","PROVISIONER_GATEWAY_PORT_80_TCP_ADDR":"10.43.127.120","KUBERNETES_SERVICE_PORT_HTTPS":"443","KUBERNETES_PORT":"tcp://10.43.0.1:443","KUBERNETES_PORT_443_TCP":"tcp://10.43.0.1:443","PROVISIONER_GATEWAY_PORT_80_TCP_PROTO":"tcp","PROVISIONER_GATEWAY_SERVICE_HOST":"10.43.127.120","PROVISIONER_GATEWAY_PORT":"tcp://10.43.127.120:80","KUBERNETES_SERVICE_HOST":"10.43.0.1","KUBERNETES_PORT_443_TCP_PROTO":"tcp","KUBERNETES_PORT_443_TCP_PORT":"443","KUBERNETES_PORT_443_TCP_ADDR":"10.43.0.1","PROVISIONER_GATEWAY_SERVICE_PORT":"80","PROVISIONER_GATEWAY_SERVICE_PORT_HTTP":"80","PROVISIONER_GATEWAY_PORT_80_TCP":"tcp://10.43.127.120:80","HOME":"/root"},"msg":"Fri, 26 Sep 2025 04:22:43 GMT | [GET] - http://10.42.0.22:80/","time":"2025-09-26T04:22:43.659Z","v":0}
{"name":"echo-server","hostname":"provisioner-gateway-fcbff6984-wc795","pid":1,"level":30,"host":{"hostname":"10.42.0.22","ip":"::ffff:10.42.0.1","ips":[]},"http":{"method":"GET","baseUrl":"","originalUrl":"/","protocol":"http"},"request":{"params":{},"query":{},"cookies":{},"body":{},"headers":{"host":"10.42.0.22:80","user-agent":"kube-probe/1.33","accept":"*/*","connection":"close"}},"environment":{"PATH":"/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin","HOSTNAME":"provisioner-gateway-fcbff6984-wc795","NODE_VERSION":"20.11.0","YARN_VERSION":"1.22.19","KUBERNETES_SERVICE_PORT":"443","PROVISIONER_GATEWAY_PORT_80_TCP_PORT":"80","PROVISIONER_GATEWAY_PORT_80_TCP_ADDR":"10.43.127.120","KUBERNETES_SERVICE_PORT_HTTPS":"443","KUBERNETES_PORT":"tcp://10.43.0.1:443","KUBERNETES_PORT_443_TCP":"tcp://10.43.0.1:443","PROVISIONER_GATEWAY_PORT_80_TCP_PROTO":"tcp","PROVISIONER_GATEWAY_SERVICE_HOST":"10.43.127.120","PROVISIONER_GATEWAY_PORT":"tcp://10.43.127.120:80","KUBERNETES_SERVICE_HOST":"10.43.0.1","KUBERNETES_PORT_443_TCP_PROTO":"tcp","KUBERNETES_PORT_443_TCP_PORT":"443","KUBERNETES_PORT_443_TCP_ADDR":"10.43.0.1","PROVISIONER_GATEWAY_SERVICE_PORT":"80","PROVISIONER_GATEWAY_SERVICE_PORT_HTTP":"80","PROVISIONER_GATEWAY_PORT_80_TCP":"tcp://10.43.127.120:80","HOME":"/root"},"msg":"Fri, 26 Sep 2025 04:22:48 GMT | [GET] - http://10.42.0.22:80/","time":"2025-09-26T04:22:48.656Z","v":0}

[1;31m‚ùå FATAL ERROR:[0m Portal end-to-end verification failed.
[1;31mDeployment failed. See /root/personal-cluster/deployment-bootstrap-20250926-001243.log for full details.[0m














































































ÊàëÂΩìÂâçÊâÄÊúâÊ∫êÁ†ÅÂ¶Ç‰∏ãÔºö


.
‚îú‚îÄ‚îÄ 01-infra
‚îÇ¬†¬† ‚îú‚îÄ‚îÄ c2-vps-setup.tf
‚îÇ¬†¬† ‚îú‚îÄ‚îÄ c3-k3s-cluster.tf
‚îÇ¬†¬† ‚îú‚îÄ‚îÄ locals.tf
‚îÇ¬†¬† ‚îú‚îÄ‚îÄ providers.tf
‚îÇ¬†¬† ‚îî‚îÄ‚îÄ variables.tf
‚îú‚îÄ‚îÄ argocd_values.md
‚îú‚îÄ‚îÄ cert_manager_Concepts_ACME_Orders_and_Challenges.md
‚îú‚îÄ‚îÄ cert_manager_Configuring_Issuers_ACME_DNS01_Cloudflare.md
‚îú‚îÄ‚îÄ cert_manager_helmcharts_values.md
‚îú‚îÄ‚îÄ cert_manager_Troubleshooting_ACME_lets_encrypt.md
‚îú‚îÄ‚îÄ collect_enhanced_diagnostics.sh
‚îú‚îÄ‚îÄ deployment-bootstrap-20250926-001243.log
‚îú‚îÄ‚îÄ deploy.sh
‚îú‚îÄ‚îÄ docker_compose_down.md
‚îú‚îÄ‚îÄ docker_network_inspect.md
‚îú‚îÄ‚îÄ docker_network_rm.md
‚îú‚îÄ‚îÄ k3s_Advanced_Options_Configuration.md
‚îú‚îÄ‚îÄ k3s_datastore_High_Availability_External_DB.md
‚îú‚îÄ‚îÄ k3s_installation_Configuration_Options.md
‚îú‚îÄ‚îÄ k3s_Networking_Basic_Network_Options.md
‚îú‚îÄ‚îÄ k3s_Networking_Networking_Services.md
‚îú‚îÄ‚îÄ k3s_releasenotes_1.33.x.md
‚îú‚îÄ‚îÄ kubernetes
‚îÇ¬†¬† ‚îú‚îÄ‚îÄ apps
‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ argocd-ingress-app.yaml
‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ authentik-app.yaml
‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ authentik-ingress-static-app.yaml
‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ cert-manager-app.yaml
‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ core-manifests-app.yaml
‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ frps-app.yaml
‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ n8n-app.yaml
‚îÇ¬†¬† ‚îÇ¬†¬† ‚îî‚îÄ‚îÄ provisioner-app.yaml
‚îÇ¬†¬† ‚îú‚îÄ‚îÄ bootstrap
‚îÇ¬†¬† ‚îÇ¬†¬† ‚îî‚îÄ‚îÄ argocd-app.yaml
‚îÇ¬†¬† ‚îî‚îÄ‚îÄ manifests
‚îÇ¬†¬†     ‚îú‚îÄ‚îÄ argocd-ingress
‚îÇ¬†¬†     ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ certificate.yaml
‚îÇ¬†¬†     ‚îÇ¬†¬† ‚îî‚îÄ‚îÄ ingress.yaml
‚îÇ¬†¬†     ‚îú‚îÄ‚îÄ authentik-ingress
‚îÇ¬†¬†     ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ certificate.yaml
‚îÇ¬†¬†     ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ ingress.yaml
‚îÇ¬†¬†     ‚îÇ¬†¬† ‚îî‚îÄ‚îÄ namespace.yaml
‚îÇ¬†¬†     ‚îú‚îÄ‚îÄ cluster-issuer
‚îÇ¬†¬†     ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ issuer.yaml
‚îÇ¬†¬†     ‚îÇ¬†¬† ‚îî‚îÄ‚îÄ secret.yaml
‚îÇ¬†¬†     ‚îú‚îÄ‚îÄ frps
‚îÇ¬†¬†     ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ config.yaml
‚îÇ¬†¬†     ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ deployment.yaml
‚îÇ¬†¬†     ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ namespace.yaml
‚îÇ¬†¬†     ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ service-and-routes.yaml
‚îÇ¬†¬†     ‚îÇ¬†¬† ‚îî‚îÄ‚îÄ wildcard-certificate.yaml
‚îÇ¬†¬†     ‚îú‚îÄ‚îÄ n8n
‚îÇ¬†¬†     ‚îÇ¬†¬† ‚îî‚îÄ‚îÄ certificate.yaml
‚îÇ¬†¬†     ‚îî‚îÄ‚îÄ provisioner
‚îÇ¬†¬†         ‚îú‚îÄ‚îÄ certificate.yaml
‚îÇ¬†¬†         ‚îú‚îÄ‚îÄ deployment.yaml
‚îÇ¬†¬†         ‚îú‚îÄ‚îÄ ingress.yaml
‚îÇ¬†¬†         ‚îú‚îÄ‚îÄ namespace.yaml
‚îÇ¬†¬†         ‚îî‚îÄ‚îÄ service.yaml
‚îú‚îÄ‚îÄ README.md
‚îú‚îÄ‚îÄ temp.sh
‚îú‚îÄ‚îÄ traefik_Reference_Configuration_Discovery_Kubernetes_CRD_HTTP_IngressRoute.md
‚îî‚îÄ‚îÄ traefik_User_Guides_Kubernetes_and_cert-manager.md

12 directories, 53 files
==> kubernetes/apps/argocd-ingress-app.yaml <==
# apps/argocd-ingress-app.yaml
apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: argocd-ingress
  namespace: argocd
  annotations:
    argocd.argoproj.io/sync-wave: "0"   # ÂèØÈÄâ
spec:
  project: default
  source:
    repoURL: https://github.com/GgYu01/personal-cluster.git
    targetRevision: HEAD
    path: kubernetes/manifests/argocd-ingress
    directory:
      recurse: true
  destination:
    server: https://kubernetes.default.svc
    namespace: argocd
  syncPolicy:
    automated:
      prune: true
      selfHeal: true
    syncOptions:
    - CreateNamespace=true
==> kubernetes/apps/authentik-app.yaml <==
# kubernetes/apps/authentik-app.yaml
# Code Analysis: Deploys Authentik using its official Helm chart, following the same
# pattern as your existing n8n-app.yaml. A sync-wave of "10" ensures it deploys
# after frps and other core services.
apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: authentik
  namespace: argocd
  annotations:
    argocd.argoproj.io/sync-wave: "10"
spec:
  project: default
  source:
    repoURL: https://charts.goauthentik.io
    chart: authentik
    targetRevision: 2024.6.1 # Pin chart version for stability
    helm:
      releaseName: authentik
      values: |
        # IMPORTANT: Replace with a long random string
        authentik:
          secret_key: "a_very_long_and_random_secret_key_change_me"

        # For simplicity, enable the bundled PostgreSQL and Redis
        postgresql:
          enabled: true
          auth:
            # IMPORTANT: Replace with a strong password
            password: "a_strong_database_password_change_me"

        redis:
          enabled: true
  destination:
    server: https://kubernetes.default.svc
    namespace: authentik
  syncPolicy:
    automated:
      prune: true
      selfHeal: true
    syncOptions:
    - CreateNamespace=true
==> kubernetes/apps/authentik-ingress-static-app.yaml <==
# apps/authentik-ingress-static-app.yaml
apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: authentik-ingress-static
  namespace: argocd
  annotations:
    argocd.argoproj.io/sync-wave: "20"   # ÂèØÈÄâ
spec:
  project: default
  source:
    repoURL: https://github.com/GgYu01/personal-cluster.git
    targetRevision: HEAD
    path: kubernetes/manifests/authentik-ingress
    directory:
      recurse: true
  destination:
    server: https://kubernetes.default.svc
    namespace: argocd
  syncPolicy:
    automated:
      prune: true
      selfHeal: true
    syncOptions:
    - CreateNamespace=true
==> kubernetes/apps/cert-manager-app.yaml <==
# kubernetes/applications/cert-manager.yaml
apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: cert-manager
  namespace: argocd
  annotations:
    argocd.argoproj.io/sync-wave: "-10"
spec:
  project: default
  source:
    repoURL: https://charts.jetstack.io
    chart: cert-manager
    targetRevision: v1.18.2 # This is the application version. The chart version will be resolved by Helm.
    helm:
      releaseName: cert-manager
      values: |
        # --- [START OF CORRECTION] ---
        # According to the provided official Helm chart documentation, 'installCRDs' is deprecated.
        # The modern and correct way to ensure CRDs are managed by the Helm chart is using 'crds.enabled'.
        crds:
          enabled: true
        # --- [END OF CORRECTION] ---
        
        startupapicheck:
          enabled: true
          # Give it ample time to succeed, especially on slower networks or busy nodes.
          timeout: 5m
        prometheus:
          enabled: false
  destination:
    server: https://kubernetes.default.svc
    namespace: cert-manager
  syncPolicy:
    automated:
      prune: true
      selfHeal: true
    syncOptions:
    - CreateNamespace=true
==> kubernetes/apps/core-manifests-app.yaml <==
# kubernetes/applications/manifests.yaml
apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: core-manifests
  namespace: argocd
spec:
  project: default
  source:
    repoURL: https://github.com/GgYu01/personal-cluster.git
    targetRevision: HEAD
    path: kubernetes/manifests/cluster-issuer
    directory:
      recurse: true
  destination:
    server: https://kubernetes.default.svc
    namespace: argocd
  syncPolicy:
    automated:
      prune: true
      selfHeal: true
    syncOptions:
    - CreateNamespace=true
==> kubernetes/apps/frps-app.yaml <==
# kubernetes/apps/frps-app.yaml
# Code Analysis: Defines the frps application for Argo CD.
# This application definition tells Argo CD to manage the resources located
# in the 'kubernetes/manifests/frps' directory. This keeps all raw manifests
# centralized, following your existing pattern. A sync-wave of "5" ensures
# it is deployed after core infrastructure like cert-manager.
apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: frps
  namespace: argocd
  annotations:
    argocd.argoproj.io/sync-wave: "5"
spec:
  project: default
  source:
    repoURL: https://github.com/GgYu01/personal-cluster.git # Using your existing repo URL
    targetRevision: HEAD
    path: kubernetes/manifests/frps # Manages all manifests within this specific path
  destination:
    server: https://kubernetes.default.svc
    namespace: frp-system
  syncPolicy:
    automated:
      prune: true
      selfHeal: true
    syncOptions:
    - CreateNamespace=true
==> kubernetes/apps/n8n-app.yaml <==
# kubernetes/apps/n8n-app.yaml
apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: n8n
  namespace: argocd
  annotations:
    # ÊâòÁÆ°È°∫Â∫èÂèØ‰øùÁïô
    argocd.argoproj.io/sync-wave: "10"
spec:
  project: default
  source:
    repoURL: https://community-charts.github.io/helm-charts
    chart: n8n
    targetRevision: 1.15.4
    helm:
      releaseName: n8n
      values: |
        # ======================================================================
        # n8n Core Configuration
        # ======================================================================
        encryptionKey: "a-very-insecure-but-static-encryption-key-for-n8n"
        timezone: "Asia/Shanghai"

        # ‰∏ªÂÆπÂô®ËÆæÁΩÆÔºàÊ≥®ÊÑèÔºöÊ≠§ chart ÁâàÊú¨‰∏çÊîØÊåÅ startupProbe/extraEnvÔºâ
        main:
          resources:
            requests:
              cpu: 10m
              memory: 32Mi
            limits:
              cpu: 500m
              memory: 512Mi

          livenessProbe:
            enabled: true
            httpGet:
              path: /healthz
              port: http
            initialDelaySeconds: 60
            periodSeconds: 10
            timeoutSeconds: 3
            failureThreshold: 12

          readinessProbe:
            enabled: true
            httpGet:
              path: /healthz/readiness
              port: http
            initialDelaySeconds: 30
            periodSeconds: 10
            timeoutSeconds: 3
            failureThreshold: 12

          # ÂÖ≥ÈîÆ‰øÆÂ§çÔºö‰ΩøÁî® map ÂΩ¢ÂºèÁöÑ extraEnvVarsÔºàËÄå‰∏çÊòØÊï∞ÁªÑÔºâ
          extraEnvVars:
            N8N_HOST: n8n.core01.prod.gglohh.top
            N8N_SECURE_COOKIE: "false"      # ‰Ω†Ë¶ÅÊ±ÇÁöÑÁ¶ÅÁî®ÂÆâÂÖ® Cookie ÈÖçÁΩÆ
            N8N_PROTOCOL: https
            N8N_PORT: "5678"
            WEBHOOK_URL: https://n8n.core01.prod.gglohh.top/

        # ‰∫åËøõÂà∂Êï∞ÊçÆÊú¨Âú∞ÊåÅ‰πÖÂåñ
        binaryData:
          mode: "filesystem"

        # ÊâßË°åÊ®°ÂºèÔºöÈòüÂàó + 1 ‰∏™ worker
        worker:
          mode: queue
          count: 1

        # Êï∞ÊçÆÂ∫ìÔºöPostgreSQL Â≠êÂõæË°®
        db:
          type: postgresdb

        postgresql:
          enabled: true
          architecture: standalone
          auth:
            database: "n8n"
            username: "admin"
            password: "password"
          primary:
            persistence:
              enabled: true
              storageClass: "local-path"
              size: 8Gi

        # ÈòüÂàóÔºöRedis Â≠êÂõæË°®
        redis:
          enabled: true
          architecture: standalone
          auth:
            enabled: true
            password: "password"
          master:
            persistence:
              enabled: true
              storageClass: "local-path"
              size: 2Gi

        # IngressÔºàTraefikÔºâ
        ingress:
          enabled: true
          className: "traefik"
          hosts:
            - host: n8n.core01.prod.gglohh.top
              paths:
                - path: /
                  pathType: Prefix
          tls:
            - secretName: n8n-tls-staging
              hosts:
                - n8n.core01.prod.gglohh.top

  destination:
    server: https://kubernetes.default.svc
    namespace: n8n
  syncPolicy:
    automated:
      prune: true
      selfHeal: true
    syncOptions:
    - CreateNamespace=true
==> kubernetes/apps/provisioner-app.yaml <==
# [ADD START] Êñ∞Â¢ûÔºöProvisioner ÁΩëÂÖ≥ÔºàÊéßÂà∂Èù¢ÔºâÔºåÁî®‰∫é‰ª§ÁâåÊ†°È™å‰∏éÂèëÊîæ Ephemeral Host
apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: provisioner
  namespace: argocd
  annotations:
    argocd.argoproj.io/sync-wave: "8"  # Âú® frps(5) ‰πãÂêé„ÄÅauthentik(10) ‰πãÂâç
spec:
  project: default
  source:
    repoURL: https://github.com/GgYu01/personal-cluster.git
    targetRevision: HEAD
    path: kubernetes/manifests/provisioner
    directory:
      recurse: true
  destination:
    server: https://kubernetes.default.svc
    namespace: provisioner
  syncPolicy:
    automated:
      prune: true
      selfHeal: true
    syncOptions:
    - CreateNamespace=true
# [ADD END]
==> kubernetes/bootstrap/argocd-app.yaml <==
# kubernetes/bootstrap/argocd-app.yaml
apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: argocd
  namespace: argocd
  finalizers:
  - resources-finalizer.argocd.argoproj.io
  annotations:
    argocd.argoproj.io/sync-options: "Replace=true"
spec:
  project: default
  source:
    repoURL: https://argoproj.github.io/argo-helm
    chart: argo-cd
    targetRevision: 8.2.7
    helm:
      releaseName: argocd
      values: |
        # ======================================================================
        # CORE CONFIGURATION
        # ======================================================================
        configs:
          # --- RBACÔºàÂÖ≥ÈîÆ‰øÆÂ§çÔºâ---
          # Ê≥®ÊÑèÔºöhelm chart ÁöÑÈîÆÊòØ policy Âíå policyDefaultÔºà‰∏çÊòØ policy.csvÔºâ
          rbac:
            create: true
            policy: |
              p, role:readonly, applications, get, */*, allow
              p, role:readonly, applications, subscribe, */*, allow
              p, role:readonly, certificates, get, *, allow
              p, role:readonly, clusters, get, *, allow
              p, role:readonly, repositories, get, *, allow
              p, role:readonly, projects, get, *, allow
              p, role:readonly, accounts, get, *, allow
              p, role:readonly, gpgkeys, get, *, allow

              p, role:admin, applications, *, */*, allow
              p, role:admin, applicationsets, *, */*, allow
              p, role:admin, certificates, *, *, allow
              p, role:admin, clusters, *, *, allow
              p, role:admin, repositories, *, *, allow
              p, role:admin, projects, *, *, allow
              p, role:admin, accounts, *, *, allow
              p, role:admin, gpgkeys, *, *, allow
              p, role:admin, exec, *, *, allow

              g, admin, role:admin
            policyDefault: ""
            scopes: '[email, groups]'
          # -----------------------

          secret:
            createSecret: true
            # admin ÂØÜÁ†ÅÔºöpassword
            argocdServerAdminPassword: "$2a$10$Xx3c/ILSzwZfp2wHhoPxFOwH4yFp3MepBtoZpR2JgTsPaG6dz1EYS"
            argocdServerAdminPasswordMtime: "2024-01-01T00:00:00Z"

        # ======================================================================
        # ARGO CD SERVER CONFIGURATION
        # ======================================================================
        server:
          extraArgs:
            - --insecure
          service:
            type: ClusterIP

        # ======================================================================
        # COMPONENT-SPECIFIC SETTINGS
        # ======================================================================
        ha:
          enabled: false
        applicationSet:
          enabled: false
        notifications:
          enabled: false
        redis:
          enabled: true
        redis-ha:
          enabled: false
  destination:
    server: https://kubernetes.default.svc
    namespace: argocd
  syncPolicy:
    automated:
      prune: true
      selfHeal: true
    syncOptions:
    - CreateNamespace=true
==> kubernetes/manifests/argocd-ingress/certificate.yaml <==
apiVersion: cert-manager.io/v1
kind: Certificate
metadata:
  name: argocd-server-tls-staging
  namespace: argocd
spec:
  secretName: argocd-server-tls-staging
  issuerRef:
    name: cloudflare-staging
    kind: ClusterIssuer
  dnsNames:
  - argocd.core01.prod.gglohh.top

==> kubernetes/manifests/argocd-ingress/ingress.yaml <==
apiVersion: traefik.io/v1alpha1
kind: IngressRoute
metadata:
  name: argocd-server-https
  namespace: argocd
spec:
  entryPoints:
    - websecure
  routes:
    - match: Host(`argocd.core01.prod.gglohh.top`)
      kind: Rule
      services:
        - name: argocd-server
          port: 80
  tls:
    secretName: argocd-server-tls-staging

==> kubernetes/manifests/authentik-ingress/certificate.yaml <==
# Code Analysis: Manages the TLS certificate for Authentik using your existing ClusterIssuer.
apiVersion: cert-manager.io/v1
kind: Certificate
metadata:
  name: authentik-tls-staging
  namespace: authentik
spec:
  secretName: authentik-tls-staging
  issuerRef:
    name: cloudflare-staging # Uses your existing issuer
    kind: ClusterIssuer
  dnsNames:
  - auth.core01.prod.gglohh.top
==> kubernetes/manifests/authentik-ingress/ingress.yaml <==
# Code Analysis: Exposes the Authentik service via Traefik, following your existing pattern.
apiVersion: traefik.io/v1alpha1
kind: IngressRoute
metadata:
  name: authentik-https
  namespace: authentik
spec:
  entryPoints:
    - websecure
  routes:
    - match: Host(`auth.core01.prod.gglohh.top`)
      kind: Rule
      services:
        # The Authentik helm chart creates a service named 'authentik-server'
        - name: authentik-server
          port: http
  tls:
    secretName: authentik-tls-staging
==> kubernetes/manifests/authentik-ingress/namespace.yaml <==
# [ADD START] ÊòéÁ°ÆÂàõÂª∫ authentik ÂëΩÂêçÁ©∫Èó¥ÔºåÈÅøÂÖçËµÑÊ∫êÂºïÁî®Â§±Ë¥•
apiVersion: v1
kind: Namespace
metadata:
  name: authentik
# [ADD END]
==> kubernetes/manifests/cluster-issuer/issuer.yaml <==
# kubernetes/manifests/cluster-issuer/issuer.yaml
apiVersion: cert-manager.io/v1
kind: ClusterIssuer
metadata:
  name: cloudflare-staging
spec:
  acme:
    email: 1405630484@qq.com
    server: https://acme-staging-v02.api.letsencrypt.org/directory
    privateKeySecretRef:
      name: letsencrypt-staging-private-key
    solvers:
    - dns01:
        cloudflare:
          # email removed when using apiTokenSecretRef (token mode)
          apiTokenSecretRef:
            name: cloudflare-api-token-secret
            key: api-token
==> kubernetes/manifests/cluster-issuer/secret.yaml <==
# kubernetes/manifests/cluster-issuer/secret.yaml
apiVersion: v1
kind: Secret
metadata:
  name: cloudflare-api-token-secret
  namespace: cert-manager
type: Opaque
stringData:
  api-token: "vi7hkPq4FwD5ttV4dvR_IoNVEJSphydRPcT0LVD-"
==> kubernetes/manifests/frps/config.yaml <==
# kubernetes/manifests/frps/config.yaml

apiVersion: v1
kind: ConfigMap
metadata:
  name: frps-config
  namespace: frp-system
data:
  frps.toml: |
    # --- Core bind ---
    bindPort = 7000

    # --- Auth (insecure by design) ---
    [auth]
    method = "token"
    token = "password"  # as requested: simple, low-security

    # --- Subdomain host for HTTP vhosts ---
    subdomainHost = "core01.prod.gglohh.top"

    # --- HTTP vhost for browser side (Traefik terminates TLS at 443) ---
    vhostHTTPPort = 8080
    # vhostHTTPSPort is not needed because TLS is terminated by Traefik

    # --- Web dashboard (insecure by design) ---
    [webServer]
    addr = "0.0.0.0"
    port = 7500
    user = "admin"
    password = "password"
==> kubernetes/manifests/frps/deployment.yaml <==
# kubernetes/manifests/frps/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: frps-server
  namespace: frp-system
spec:
  replicas: 1
  selector:
    matchLabels:
      app: frps
  template:
    metadata:
      labels:
        app: frps
    spec:
      containers:
      - name: frps
        image: snowdreamtech/frps:0.58.1
        args: ["-c", "/etc/frp/frps.toml"]
        ports:
        - containerPort: 7000
          name: frps-bind
        - containerPort: 8080
          name: vhost-http
        - containerPort: 7500
          name: dashboard
        # envFrom removed (no longer needed)
        volumeMounts:
        - name: config-volume
          mountPath: /etc/frp
          readOnly: true
      volumes:
      - name: config-volume
        configMap:
          name: frps-config
==> kubernetes/manifests/frps/namespace.yaml <==
# Code Analysis: Creates a dedicated namespace for frps to keep resources isolated.
apiVersion: v1
kind: Namespace
metadata:
  name: frp-system
==> kubernetes/manifests/frps/service-and-routes.yaml <==
# kubernetes/manifests/frps/service-and-routes.yaml
apiVersion: v1
kind: Service
metadata:
  name: frps-service
  namespace: frp-system
spec:
  selector:
    app: frps
  ports:
  - name: frps-bind
    port: 7000
  - name: vhost-http
    port: 8080
  - name: dashboard
    port: 7500
---
apiVersion: traefik.io/v1alpha1
kind: IngressRouteTCP
metadata:
  name: frps-tcp-ingress
  namespace: frp-system
spec:
  entryPoints:
    - frps  # must exist in Traefik static config; deploy.sh will ensure it
  routes:
  - match: HostSNI(`*`)
    services:
    - name: frps-service
      port: 7000
---
apiVersion: traefik.io/v1alpha1
kind: IngressRoute
metadata: 
  name: frps-http-ingress
  namespace: frp-system
  annotations:
    # [ADD START]
    argocd.argoproj.io/sync-wave: "1"
    # [ADD END]
spec:
  entryPoints: 
    - websecure
  routes: 
  - kind: Rule
    match: HostRegexp(`e-{subdomain:[a-z0-9-]+}\.core01\.prod\.gglohh\.top`)
    services: 
    - name: frps-service
      port: 8080
  tls: 
    secretName: wildcard-core01-prod-gglohh-top-tls

==> kubernetes/manifests/frps/wildcard-certificate.yaml <==
# ‰øÆÊîπËåÉÂõ¥ÔºöÂ¢ûÂä†ÂêåÊ≠•Ê≥¢Ê¨°Ê≥®Ëß£ÔºåÁ°Æ‰øùËØÅ‰π¶ÂÖà‰∫é IngressRoute
apiVersion: cert-manager.io/v1
kind: Certificate
metadata:
  name: wildcard-core01-prod-gglohh-top
  namespace: frp-system
  annotations:
    # [ADD START]
    argocd.argoproj.io/sync-wave: "0"
    # [ADD END]
spec:
  secretName: wildcard-core01-prod-gglohh-top-tls
  issuerRef:
    name: cloudflare-staging
    kind: ClusterIssuer
  dnsNames:
  - "*.core01.prod.gglohh.top"
==> kubernetes/manifests/n8n/certificate.yaml <==
# kubernetes/manifests/n8n/certificate.yaml
apiVersion: cert-manager.io/v1
kind: Certificate
metadata:
  name: n8n-tls-staging
  namespace: n8n # MUST be in the same namespace as the Ingress
spec:
  secretName: n8n-tls-staging # This name is referenced by the Ingress created by the Helm chart
  issuerRef:
    name: cloudflare-staging
    kind: ClusterIssuer
  dnsNames:
  - n8n.core01.prod.gglohh.top
==> kubernetes/manifests/provisioner/certificate.yaml <==
# ‰øÆÊîπËåÉÂõ¥ÔºöÂú® metadata Â¢ûÂä† Argo CD ÂêåÊ≠•Ê≥¢Ê¨°Ê≥®Ëß£ÔºàËØÅ‰π¶ÂÖà‰∫é IngressRouteÔºâ
apiVersion: cert-manager.io/v1
kind: Certificate
metadata:
  name: portal-tls-staging
  namespace: provisioner
  annotations:
    # [ADD START] ensure cert before ingress
    argocd.argoproj.io/sync-wave: "0"
    # [ADD END]
spec:
  secretName: portal-tls-staging
  issuerRef:
    name: cloudflare-staging
    kind: ClusterIssuer
  dnsNames:
  - portal.core01.prod.gglohh.top
==> kubernetes/manifests/provisioner/deployment.yaml <==
# ‰øÆÊîπËåÉÂõ¥ÔºöÊï¥ÊÆµ container ÈÖçÁΩÆÔºàÊõøÊç¢ÈïúÂÉè‰∏∫ÂèØÂÖ¨ÂºÄÊãâÂèñÁöÑ echo-serverÔºâ
apiVersion: apps/v1
kind: Deployment
metadata:
  name: provisioner-gateway
  namespace: provisioner
spec:
  replicas: 1
  selector:
    matchLabels:
      app: provisioner-gateway
  template:
    metadata:
      labels:
        app: provisioner-gateway
    spec:
      containers:
      - name: gateway
        image: ealen/echo-server:latest
        imagePullPolicy: IfNotPresent
        ports:
        - containerPort: 80
        readinessProbe:
          httpGet:
            path: /
            port: 80
          initialDelaySeconds: 2
          periodSeconds: 5
          timeoutSeconds: 2
          failureThreshold: 3
==> kubernetes/manifests/provisioner/ingress.yaml <==
# ‰øÆÊîπËåÉÂõ¥ÔºöÂú® metadata Â¢ûÂä† Argo CD ÂêåÊ≠•Ê≥¢Ê¨°Ê≥®Ëß£ÔºàIngressRoute Êôö‰∫éËØÅ‰π¶Ôºâ
apiVersion: traefik.io/v1alpha1
kind: IngressRoute
metadata:
  name: provisioner-https
  namespace: provisioner
  annotations:
    # [ADD START] ensure ingress after cert
    argocd.argoproj.io/sync-wave: "1"
    # [ADD END]
spec:
  entryPoints:
    - websecure
  routes:
  - match: Host(`portal.core01.prod.gglohh.top`) && PathPrefix(`/`)
    kind: Rule
    services:
    - name: provisioner-gateway
      port: 80
  tls:
    secretName: portal-tls-staging
==> kubernetes/manifests/provisioner/namespace.yaml <==
# [ADD START]
apiVersion: v1
kind: Namespace
metadata:
  name: provisioner
# [ADD END]
==> kubernetes/manifests/provisioner/service.yaml <==
# [ADD START]
apiVersion: v1
kind: Service
metadata:
  name: provisioner-gateway
  namespace: provisioner
spec:
  selector:
    app: provisioner-gateway
  ports:
  - name: http
    port: 80
    targetPort: 8080
# [ADD END]
==> ./deploy.sh <==
#!/usr/bin/env bash

# ==============================================================================
#
#       PERSONAL CLUSTER DEPLOYMENT BOOTSTRAPPER (v23.1 - Static Password)
#
# ==============================================================================
#
#   VERSION 23.1 CHANGE LOG:
#   - PASSWORD MANAGEMENT: Implemented static password for Argo CD 'admin' user
#     based on official Helm chart v8.2.7 documentation. The password is now
#     consistently 'password'.
#   - BOOTSTRAP PROCESS: Modified both the initial 'helm install' command and the
#     GitOps Application manifest (`argocd-app.yaml`) to include the bcrypt-hashed
#     password. This ensures correctness at creation and prevents configuration
#     drift during self-healing.
#   - VERIFICATION: Removed logic for retrieving a random password. The final
#     output now displays the static credentials.
#
# ==============================================================================

set -eo pipefail

# --- [SECTION 1: CONFIGURATION VARIABLES] ---
readonly VPS_IP="172.245.187.113"
readonly DOMAIN_NAME="gglohh.top"
readonly SITE_CODE="core01"
readonly ENVIRONMENT="prod"
readonly K3S_CLUSTER_TOKEN="admin" # Simple, as requested
readonly ARGOCD_ADMIN_PASSWORD="password"

# --- ACME & DNS (Cloudflare) ---
# NOTE: Hard-coded, insecure by design, as requested
readonly ACME_EMAIL="1405630484@qq.com"
readonly CF_API_TOKEN="vi7hkPq4FwD5ttV4dvR_IoNVEJSphydRPcT0LVD-"
readonly WILDCARD_FQDN="*.${SITE_CODE}.${ENVIRONMENT}.${DOMAIN_NAME}"   # *.core01.prod.gglohh.top
readonly CF_PROXIED="false"

# --- Software Versions ---
readonly K3S_VERSION="v1.33.3+k3s1"

# --- Internal Settings ---
readonly ETCD_PROJECT_NAME="personal-cluster-etcd"
readonly ETCD_CONTAINER_NAME="core-etcd"
readonly ETCD_DATA_DIR="/opt/etcd/data"
readonly ETCD_CONTAINER_USER_ID=1001
readonly ETCD_NETWORK_NAME="${ETCD_PROJECT_NAME}_default"
readonly KUBECONFIG_PATH="/etc/rancher/k3s/k3s.yaml"
readonly USER_KUBECONFIG_PATH="${HOME}/.kube/config"
readonly TIMESTAMP=$(date +"%Y%m%d-%H%M%S")
readonly LOG_FILE_NAME="deployment-bootstrap-${TIMESTAMP}.log"
readonly LOG_FILE="$(pwd)/${LOG_FILE_NAME}"
readonly ARGOCD_FQDN="argocd.${SITE_CODE}.${ENVIRONMENT}.${DOMAIN_NAME}"
readonly PORTAL_FQDN="portal.${SITE_CODE}.${ENVIRONMENT}.${DOMAIN_NAME}"  # portal.core01.prod.gglohh.top
readonly KUBELET_CONFIG_PATH="/etc/rancher/k3s/kubelet.config"

# --- [START OF PASSWORD FIX] ---
# Statically define the bcrypt hash for the password 'password'.
# This avoids re-calculating it on every run and makes the script's intent clearer.
readonly ARGOCD_ADMIN_PASSWORD_HASH='$2a$10$Xx3c/ILSzwZfp2wHhoPxFOwH4yFp3MepBtoZpR2JgTsPaG6dz1EYS'
# --- [END OF PASSWORD FIX] ---

# --- [NEW - SECTION 2.1: Cloudflare DNS Helpers] ---
# Purpose: Manage wildcard A record state-driven via CF API (idempotent, non-interactive).
cf_api() {
  # $1: method, $2: path, $3: data (optional)
  local method="$1"; local path="$2"; local data="${3:-}"
  if [[ -n "$data" ]]; then
    curl -sS -X "${method}" "https://api.cloudflare.com/client/v4${path}" \
      -H "Authorization: Bearer ${CF_API_TOKEN}" \
      -H "Content-Type: application/json" \
      --data-raw "${data}"
  else
    curl -sS -X "${method}" "https://api.cloudflare.com/client/v4${path}" \
      -H "Authorization: Bearer ${CF_API_TOKEN}" \
      -H "Content-Type: application/json"
  fi
}

wait_apiserver_ready() {
  # $1 timeout seconds (default 180), $2 interval seconds (default 5)
  local timeout_s="${1:-180}"
  local interval_s="${2:-5}"
  log_info "Checking Kubernetes apiserver readiness (/readyz) with timeout ${timeout_s}s..."
  if ! timeout "${timeout_s}s" bash -lc \
    'until kubectl --request-timeout=10s get --raw=/readyz >/dev/null 2>&1; do echo "    ...apiserver not ready yet"; sleep '"${interval_s}"'; done'; then
    log_error_and_exit "Kubernetes apiserver is not ready within ${timeout_s}s."
  fi
  log_success "Kubernetes apiserver reports Ready."
}

ensure_cloudflare_wildcard_a() {
  # Non-interactive, state-driven; creates or updates wildcard A only when needed.
  log_step 0 "Ensure Cloudflare wildcard DNS"
  local zone_name="${DOMAIN_NAME}"
  local sub_wildcard="*.${SITE_CODE}.${ENVIRONMENT}.${DOMAIN_NAME}"

  log_info "Resolving Cloudflare Zone ID for ${zone_name} ..."
  local zone_resp; zone_resp=$(cf_api GET "/zones?name=${zone_name}")
  local zone_id; zone_id=$(echo "${zone_resp}" | jq -r '.result[0].id')
  if [[ -z "${zone_id}" || "${zone_id}" == "null" ]]; then
    log_error_and_exit "Cloudflare zone '${zone_name}' not found."
  fi
  log_success "Cloudflare Zone ID acquired: ${zone_id}"

  log_info "Checking existing DNS record for ${sub_wildcard} (type A) ..."
  # URL-encode "*." as %2A.
  local qname; qname="%2A.${SITE_CODE}.${ENVIRONMENT}.${DOMAIN_NAME}"
  local rec_resp; rec_resp=$(cf_api GET "/zones/${zone_id}/dns_records?type=A&name=${qname}")
  local rec_id; rec_id=$(echo "${rec_resp}" | jq -r '.result[0].id // empty')
  local rec_ip; rec_ip=$(echo "${rec_resp}" | jq -r '.result[0].content // empty')

  if [[ -n "${rec_id}" ]]; then
    if [[ "${rec_ip}" == "${VPS_IP}" ]]; then
      log_success "Wildcard A already correct: ${sub_wildcard} -> ${VPS_IP} (no action)."
    else
      log_info "Updating wildcard A to ${VPS_IP} ..."
      local payload; payload=$(jq -nc --arg name "${sub_wildcard}" --arg ip "${VPS_IP}" --argjson proxied ${CF_PROXIED} \
        '{type:"A", name:$name, content:$ip, ttl:1, proxied:$proxied}')
      local up_resp; up_resp=$(cf_api PUT "/zones/${zone_id}/dns_records/${rec_id}" "${payload}")
      if [[ "$(echo "${up_resp}" | jq -r '.success')" != "true" ]]; then
        echo "${up_resp}" | sed 's/^/CF-ERR: /g'
        log_error_and_exit "Failed to update wildcard A record."
      fi
      log_success "Wildcard A updated: ${sub_wildcard} -> ${VPS_IP}"
    fi
  else
    log_info "Creating wildcard A ${sub_wildcard} -> ${VPS_IP} ..."
    local payload; payload=$(jq -nc --arg name "${sub_wildcard}" --arg ip "${VPS_IP}" --argjson proxied ${CF_PROXIED} \
      '{type:"A", name:$name, content:$ip, ttl:1, proxied:$proxied}')
    local cr_resp; cr_resp=$(cf_api POST "/zones/${zone_id}/dns_records" "${payload}")
    if [[ "$(echo "${cr_resp}" | jq -r '.success')" != "true" ]]; then
      echo "${cr_resp}" | sed 's/^/CF-ERR: /g'
      log_error_and_exit "Failed to create wildcard A record."
    fi
    log_success "Wildcard A created: ${sub_wildcard} -> ${VPS_IP}"
  fi

  log_info "Verifying public resolution via 1.1.1.1 ..."
  local probe_fqdn="test.${SITE_CODE}.${ENVIRONMENT}.${DOMAIN_NAME}"
  if ! timeout 60 bash -lc "until dig +short @1.1.1.1 ${probe_fqdn} A | grep -q '^${VPS_IP}\$'; do echo '    ...waiting DNS...'; sleep 5; done"; then
    log_warn "Public resolution for ${probe_fqdn} did not return ${VPS_IP} within timeout."
  else
    log_success "Public DNS resolution OK for wildcard subdomain."
  fi
}

# --- [SECTION 2: LOGGING & DIAGNOSTICS] ---
log_step() { printf "\n\n\033[1;34m# ============================================================================== #\033[0m\n"; printf "\033[1;34m# STEP %s: %s (Timestamp: %s)\033[0m\n" "$1" "$2" "$(date -u --iso-8601=seconds)"; printf "\033[1;34m# ============================================================================== #\033[0m\n\n"; }
log_info() { echo "--> INFO: $1"; }
log_warn() { echo -e "\033[1;33m‚ö†Ô∏è  WARN: $1\033[0m"; }
log_success() { echo -e "\033[1;32m‚úÖ SUCCESS:\033[0m $1"; }
log_error_and_exit() { echo -e "\n\033[1;31m‚ùå FATAL ERROR:\033[0m $1" >&2; echo -e "\033[1;31mDeployment failed. See ${LOG_FILE} for full details.\033[0m" >&2; exit 1; }

run_with_retry() {
    local cmd="$1"
    local description="$2"
    local timeout_seconds="$3"
    local interval_seconds="${4:-10}"
    
    log_info "Verifying: ${description} (Timeout: ${timeout_seconds}s)"
    if ! timeout "${timeout_seconds}s" bash -c -- "until ${cmd} &>/dev/null; do printf '    ...waiting...\\n'; sleep ${interval_seconds}; done"; then
        log_warn "Condition '${description}' was NOT met within the timeout period."
        return 1
    fi
    log_success "Verified: ${description}."
    return 0
}

# Single-shot job logs collector for helm jobs
print_job_pod_logs() {
    # $1: namespace, $2: job name
    local ns="$1"; local job="$2"
    echo "==== [DIAG] Logs for Job ${ns}/${job} ===="
    local pods
    pods=$(kubectl -n "${ns}" get pods --selector=job-name="${job}" -o jsonpath='{.items[*].metadata.name}' 2>/dev/null || true)
    if [[ -z "${pods}" ]]; then
        echo "(no pods found for job ${job})"
        return 0
    fi
    for p in ${pods}; do
        echo "--- Pod: ${p} (containers) ---"
        kubectl -n "${ns}" get pod "${p}" -o jsonpath='{.spec.containers[*].name}' 2>/dev/null || true
        echo
        for c in $(kubectl -n "${ns}" get pod "${p}" -o jsonpath='{.spec.containers[*].name}' 2>/dev/null); do
            echo "----- Container: ${c} -----"
            kubectl -n "${ns}" logs "${p}" -c "${c}" --tail=500 2>/dev/null || true
        done
    done
}

# Wait helm install job to succeed; no early-exit on BackOff (controller may retry)
wait_helm_job_success() {
    # $1: namespace, $2: job name, $3: timeout seconds
    local ns="$1"; local job="$2"; local timeout_s="$3"
    log_info "Waiting Job ${ns}/${job} to succeed..."
    if ! timeout "${timeout_s}s" bash -lc "until [[ \$(kubectl -n ${ns} get job ${job} -o jsonpath='{.status.succeeded}' 2>/dev/null) == 1 ]]; do sleep 5; done"; then
        log_warn "Timeout waiting Job ${ns}/${job} to succeed."
        print_job_pod_logs "${ns}" "${job}"
        return 1
    fi
    log_success "Job ${ns}/${job} succeeded."
    return 0
}

# Wait all required Traefik CRDs available after traefik-crd job
wait_for_traefik_crds() {
    log_info "Waiting for Traefik CRDs to be established in API..."
    local crds=(
        ingressroutes.traefik.io
        ingressroutetcps.traefik.io
        ingressrouteudps.traefik.io
        middlewares.traefik.io
        traefikservices.traefik.io
        tlsoptions.traefik.io
        serverstransports.traefik.io
    )
    for c in "${crds[@]}"; do
        if ! run_with_retry "kubectl get crd ${c} >/dev/null 2>&1" "CRD ${c} present" 180 5; then
            log_error_and_exit "Required CRD ${c} not found after traefik-crd installation."
        fi
    done
    log_success "All Traefik CRDs are present."
}

# [New] Dump HelmChart & HelmChartConfig valuesContent once for high-value diagnostics
diagnose_traefik_values_merge() {
    echo "==== [DIAG] HelmChart kube-system/traefik (spec.valuesContent) ===="
    kubectl -n kube-system get helmchart traefik -o jsonpath='{.spec.valuesContent}' 2>/dev/null || true
    echo
    echo "==== [DIAG] HelmChartConfig kube-system/traefik (spec.valuesContent) ===="
    kubectl -n kube-system get helmchartconfig traefik -o jsonpath='{.spec.valuesContent}' 2>/dev/null || true
    echo
    echo "==== [DIAG] Traefik Service (full manifest) ===="
    kubectl -n kube-system get svc traefik -o yaml 2>/dev/null || true
}

# --- New: compact diagnostic for Traefik installation ---
diagnose_traefik_install() {
    # Single-shot diagnostics, no loops. Minimal but high-value.
    echo "==== [DIAG] kube-system basic resources ===="
    kubectl -n kube-system get deploy,po,svc,helmchart 2>/dev/null || true

    echo "==== [DIAG] helm-controller status ===="
    kubectl -n kube-system get deploy/helm-controller -o yaml 2>/dev/null || true
    kubectl -n kube-system logs deploy/helm-controller --tail=100 2>/dev/null || true

    echo "==== [DIAG] HelmChart traefik (if any) ===="
    kubectl -n kube-system get helmchart traefik -o yaml 2>/dev/null || true

    echo "==== [DIAG] Traefik deployment (if any) ===="
    kubectl -n kube-system get deploy/traefik -o yaml 2>/dev/null || true
    kubectl -n kube-system logs deploy/traefik --tail=200 2>/dev/null || true

    echo "==== [DIAG] Traefik service (if any) ===="
    kubectl -n kube-system get svc/traefik -o yaml 2>/dev/null || true

    echo "==== [DIAG] Recent kube-system events ===="
    kubectl -n kube-system get events --sort-by=.lastTimestamp | tail -n 50 2>/dev/null || true
}

# --- [SECTION 3: DEPLOYMENT FUNCTIONS] ---

function perform_system_cleanup() {
    log_step 1 "System Cleanup"
    log_info "This step will eradicate all traces of previous K3s and this project's ETCD."
    
    log_info "Stopping k3s, docker, and containerd services..."
    systemctl stop k3s.service &>/dev/null || true
    systemctl disable k3s.service &>/dev/null || true
    
    if command -v docker &>/dev/null && systemctl is-active --quiet docker.service; then
        log_info "Forcefully removing project's ETCD container and network..."
        docker rm -f "${ETCD_CONTAINER_NAME}" &>/dev/null || true
        docker network rm "${ETCD_NETWORK_NAME}" &>/dev/null || true
    else
        log_warn "Docker not running or not installed. Skipping Docker resource cleanup."
    fi
    
    log_info "Running K3s uninstaller and cleaning up filesystem..."
    if [ -x /usr/local/bin/k3s-uninstall.sh ]; then
        /usr/local/bin/k3s-uninstall.sh &>/dev/null
    fi
    rm -rf /var/lib/rancher/k3s /etc/rancher /var/lib/kubelet /run/flannel /run/containerd /var/lib/containerd /tmp/k3s-*
    rm -rf "${ETCD_DATA_DIR}"
    rm -f /etc/systemd/system/k3s.service /etc/systemd/system/k3s.service.env "${KUBELET_CONFIG_PATH}" "${KUBECONFIG_PATH}"
    rm -rf "${HOME}/.kube"

    log_info "Reloading systemd and cleaning journals for k3s and docker..."
    systemctl daemon-reload
    # --- [MODIFY START] targeted journal handling (do not vacuum unrelated logs) ---
    log_info "Rotating systemd journal only (no global vacuum; keep unrelated services' logs)..."
    journalctl --rotate || true
    # --- [MODIFY END] ---
    
    log_success "System cleanup complete."
}

function deploy_etcd() {
    log_step 2 "Deploy and Verify External ETCD"
    
    log_info "Preparing ETCD data directory with correct permissions for UID ${ETCD_CONTAINER_USER_ID}..."
    mkdir -p "${ETCD_DATA_DIR}"
    chown -R "${ETCD_CONTAINER_USER_ID}:${ETCD_CONTAINER_USER_ID}" "${ETCD_DATA_DIR}"
    
    log_info "Deploying ETCD via Docker..."
    docker run -d --restart unless-stopped \
      -p 127.0.0.1:2379:2379 \
      -v "${ETCD_DATA_DIR}":/bitnami/etcd/data \
      --name "${ETCD_CONTAINER_NAME}" \
      -e ALLOW_NONE_AUTHENTICATION=yes \
      bitnami/etcd:latest >/dev/null
      
    log_success "ETCD container started."

    if ! run_with_retry "curl --fail --silent http://127.0.0.1:2379/health" "ETCD to be healthy" 60 5; then
        log_info "ETCD health check failed. Dumping container logs for diagnosis:"
        docker logs "${ETCD_CONTAINER_NAME}"
        log_error_and_exit "ETCD deployment failed."
    fi
}

function install_k3s() {
    log_step 3 "Install and Verify K3S"

    log_info "Preparing K3s manifest and configuration directories..."
    mkdir -p /var/lib/rancher/k3s/server/manifests
    mkdir -p "$(dirname "${KUBELET_CONFIG_PATH}")"

  log_info "Creating Traefik HelmChartConfig with CRD provider and frps (7000/TCP) entryPoint..."
  cat > /var/lib/rancher/k3s/server/manifests/traefik-config.yaml << 'EOF'
apiVersion: helm.cattle.io/v1
kind: HelmChartConfig
metadata:
  name: traefik
  namespace: kube-system
spec:
  valuesContent: |-
    # ÂêØÁî® CRD/Ingress ‰∏§Á±ª providerÔºåÂπ∂ËÆ© Ingress ‰ΩøÁî®Â∑≤ÂèëÂ∏ÉÁöÑÊúçÂä°ÂÅöÁä∂ÊÄÅÂõûÂ°´
    providers:
      kubernetesCRD:
        enabled: true
      kubernetesIngress:
        publishedService:
          enabled: true

    # Ê≠£Á°ÆÁöÑÂÖ•Âè£ÁÇπÂÆö‰πâÔºàÊØè‰∏™ÂÖ•Âè£ÁÇπÂÜÖ‰ΩøÁî® expose.defaultÔºâ
    ports:
      web:
        port: 8000
        exposedPort: 80
        expose:
          default: true
      websecure:
        port: 8443
        exposedPort: 443
        expose:
          default: true
      frps:
        port: 7000
        exposedPort: 7000
        protocol: TCP
        expose:
          default: true

    # ‰∏∫ websecure ÊòæÂºèÂêØÁî® TLSÔºàÁî± IngressRoute/Certificate ÊéßÂà∂ËØÅ‰π¶Ôºâ
    additionalArguments:
      - "--entrypoints.websecure.http.tls=true"
EOF

    cat > "${KUBELET_CONFIG_PATH}" << EOF
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
failSwapOn: false
EOF
    log_success "K3s customization manifests created."

    log_info "Installing K3s ${K3S_VERSION}..."
    local install_cmd=(
        "curl -sfL https://get.k3s.io |"
        "INSTALL_K3S_VERSION='${K3S_VERSION}'"
        "K3S_TOKEN='${K3S_CLUSTER_TOKEN}'"
        "sh -s - server"
        "--cluster-init"
        "--datastore-endpoint='http://127.0.0.1:2379'"
        "--tls-san='${VPS_IP}'"
        "--flannel-backend=host-gw"
        "--kubelet-arg='config=${KUBELET_CONFIG_PATH}'"
    )
    eval "${install_cmd[*]}"
    log_success "K3s installation script finished."

    log_info "Setting up kubeconfig for user..."
    mkdir -p "$(dirname "${USER_KUBECONFIG_PATH}")"
    cp "${KUBECONFIG_PATH}" "${USER_KUBECONFIG_PATH}"
    chown "$(id -u):$(id -g)" "${USER_KUBECONFIG_PATH}"
    export KUBECONFIG="${USER_KUBECONFIG_PATH}"

    if ! run_with_retry "kubectl get node $(hostname | tr '[:upper:]' '[:lower:]') --no-headers | awk '{print \$2}' | grep -q 'Ready'" "K3s node to be Ready" 180; then
        log_info "K3s node did not become ready. Dumping K3s service logs:"
        journalctl -u k3s.service --no-pager -n 500
        log_error_and_exit "K3s cluster verification failed."
    fi

    # Wait for k3s HelmChart resources
    log_info "Waiting for HelmChart 'traefik' to appear..."
    if ! run_with_retry "kubectl -n kube-system get helmchart traefik >/dev/null 2>&1" "HelmChart/traefik exists" 240 5; then
        log_error_and_exit "HelmChart 'traefik' not found; Traefik installation not started."
    fi
    log_success "HelmChart/traefik detected."

    # Wait CRD job success then ensure CRDs visible at API level
    log_info "Waiting for job/helm-install-traefik-crd to succeed..."
    if ! wait_helm_job_success "kube-system" "helm-install-traefik-crd" 360; then
        log_error_and_exit "job/helm-install-traefik-crd failed."
    fi
    wait_for_traefik_crds

    # Wait Traefik install job success (controller will retry if first attempt failed)
    log_info "Waiting for job/helm-install-traefik to succeed..."
    if ! wait_helm_job_success "kube-system" "helm-install-traefik" 600; then
        log_error_and_exit "job/helm-install-traefik failed."
    fi

    # Deployment + Service port verification
    log_info "Waiting for Traefik Deployment to be created..."
    if ! run_with_retry "kubectl -n kube-system get deploy traefik >/dev/null 2>&1" "Deployment/traefik exists" 240 5; then
        log_error_and_exit "Traefik Deployment not created."
    fi
    log_info "Waiting for Traefik Deployment rollout..."
    if ! run_with_retry "kubectl -n kube-system rollout status deploy/traefik --timeout=90s" "Traefik Deployment rollout" 480 10; then
        log_error_and_exit "Traefik Deployment failed to roll out."
    fi
    log_success "Traefik Deployment is Ready."

    log_info "Checking Service/traefik exposes required ports (80, 443, 7000)..."
    local ports_cmd="kubectl -n kube-system get svc traefik -o jsonpath='{.spec.ports[*].port}' | tr ' ' '\n' | sort -n | tr '\n' ' '"
    if ! run_with_retry "${ports_cmd} | grep -Eq '\b80\b' && ${ports_cmd} | grep -Eq '\b443\b' && ${ports_cmd} | grep -Eq '\b7000\b'" "Service/traefik to expose 80,443,7000" 300 10; then
        echo "Observed ports: $(eval ${ports_cmd} 2>/dev/null || true)"
        diagnose_traefik_values_merge
        log_error_and_exit "Traefik Service does not expose required ports."
    fi
    log_success "Traefik Service exposes 80/443/7000."
}

# --- New: verify frps entryPoint & wildcard TLS readiness ---
function verify_frps_entrypoint_and_tls() {
    log_step 6 "Verify frps entryPoint listening and wildcard TLS readiness"

    # 1) Verify IngressRouteTCP exists and references entryPoint 'frps'
    if ! run_with_retry "kubectl -n frp-system get ingressroutetcp frps-tcp-ingress >/dev/null 2>&1" "IngressRouteTCP 'frps-tcp-ingress' present" 120 5; then
        log_info "Dumping IngressRouteTCP list in frp-system:"
        kubectl -n frp-system get ingressroutetcp -o yaml || true
        log_error_and_exit "IngressRouteTCP 'frps-tcp-ingress' not found."
    fi
    log_success "IngressRouteTCP 'frps-tcp-ingress' is present."

    # 2) Verify Traefik service exposes 7000 and external TCP is reachable
    #    This uses a TCP connect check against VPS_IP:7000
    local tcp_check_cmd="timeout 2 bash -lc '</dev/tcp/${VPS_IP}/7000' >/dev/null 2>&1"
    if ! run_with_retry "${tcp_check_cmd}" "External TCP connectivity to ${VPS_IP}:7000" 180 5; then
        log_info "Failed TCP connect to ${VPS_IP}:7000. Dumping diagnostics:"
        kubectl -n kube-system get svc traefik -o wide || true
        kubectl -n kube-system get pods -l app.kubernetes.io/name=traefik -o wide || true
        kubectl -n kube-system logs -l app.kubernetes.io/name=traefik --tail=100 || true
        log_warn "Possible external firewall or provider-level filtering on port 7000."
        log_error_and_exit "frps entryPoint is not externally reachable on ${VPS_IP}:7000."
    fi
    log_success "frps entryPoint is externally reachable on ${VPS_IP}:7000."

    # 3) Verify wildcard Certificate in frp-system namespace
    #    Name should match your manifest (e.g. wildcard-core01-prod-gglohh-top)
    local cert_name="wildcard-core01-prod-gglohh-top"
    if ! kubectl -n frp-system get certificate "${cert_name}" >/dev/null 2>&1; then
        log_warn "Certificate '${cert_name}' not found in namespace 'frp-system'. Skipping Ready wait."
        log_info "List certificates in frp-system for reference:"
        kubectl -n frp-system get certificate || true
    else
        if ! run_with_retry "kubectl -n frp-system wait --for=condition=Ready certificate/${cert_name} --timeout=100s" "Wildcard Certificate '${cert_name}' to be Ready" 320 10; then
            log_info "Certificate not Ready. Dumping certificate and cert-manager logs:"
            kubectl -n frp-system describe certificate "${cert_name}" || true
            kubectl -n cert-manager logs -l app.kubernetes.io/instance=cert-manager --all-containers --tail=100 || true
            log_error_and_exit "Wildcard certificate '${cert_name}' not Ready."
        fi
        log_success "Wildcard certificate '${cert_name}' is Ready."
    fi

    # 4) Verify TLS Secret exists for Traefik IngressRoute usage
    local tls_secret="wildcard-core01-prod-gglohh-top-tls"
    if ! run_with_retry "kubectl -n frp-system get secret ${tls_secret} >/dev/null 2>&1" "TLS Secret '${tls_secret}' present in frp-system" 120 5; then
        log_info "Dumping secrets in frp-system:"
        kubectl -n frp-system get secrets || true
        log_error_and_exit "TLS Secret '${tls_secret}' is missing in frp-system."
    fi
    log_success "TLS Secret '${tls_secret}' present in frp-system (for HTTPS on wildcard)."
}

function bootstrap_gitops() {
    log_step 4 "Bootstrap GitOps Engine (Argo CD)"

    log_info "Bootstrapping Argo CD via Helm..."
    log_info "This initial install will create CRDs and components with static credentials."

    helm repo add argo https://argoproj.github.io/argo-helm &>/dev/null || helm repo update

    # --- [START OF PASSWORD FIX] ---
    # Inject the bcrypt-hashed password and the '--insecure' flag during the initial Helm install.
    # This ensures the 'argocd-secret' is created with the correct static password from the very beginning,
    # preventing the creation of the 'argocd-initial-admin-secret' with a random password.
    helm upgrade --install argocd argo/argo-cd \
        --version 8.2.7 \
        --namespace argocd --create-namespace \
        --set-string "server.extraArgs={--insecure}" \
        --set-string "configs.secret.argocdServerAdminPassword=${ARGOCD_ADMIN_PASSWORD_HASH}" \
        --set-string "configs.secret.argocdServerAdminPasswordMtime=$(date -u --iso-8601=seconds)" \
        --wait --timeout=15m
    # --- [END OF PASSWORD FIX] ---

    log_success "Argo CD components and CRDs installed via Helm with static password."

    log_info "Applying Argo CD application manifests to enable GitOps self-management..."
    kubectl apply -f kubernetes/bootstrap/argocd-app.yaml

    log_info "Waiting for Argo CD to sync its own application resource..."
    if ! run_with_retry "kubectl get application/argocd -n argocd -o jsonpath='{.status.health.status}' | grep -q 'Healthy'" "Argo CD to become Healthy and self-managed" 300; then
        log_info "Argo CD self-management sync failed. Dumping application status:"
        kubectl get application/argocd -n argocd -o yaml
        log_error_and_exit "Argo CD bootstrap failed at self-management step."
    fi

    log_success "Argo CD has been bootstrapped and is now self-managing via GitOps."
}

function deploy_applications() {
    log_step 5 "Deploy Core Applications via GitOps"

    # 1) ‰ªÖÊèê‰∫§ cert-manager Application
    log_info "Applying cert-manager Application (only)..."
    kubectl apply -f kubernetes/apps/cert-manager-app.yaml

    # 2) API server Â∞±Áª™Èó®ÊéßÔºåÈÅøÂÖç Admission Ê≥®ÂÜåËøáÁ®ãÁöÑÁû¨Êó∂Â§±Ë¥•
    wait_apiserver_ready 300 5

    # 3) Á≠âÂæÖ cert-manager Ê†∏ÂøÉ Deployment ÂÆûÈôÖÂ∞±Áª™ÔºàÊØîÁõ¥Êé•Áúã Argo Application Êõ¥Ë¥¥Ëøë‰∫ãÂÆûÔºâ
    log_info "Waiting for cert-manager Deployments to become Available..."
    timeout 600 bash -lc 'until kubectl -n cert-manager get deploy cert-manager cert-manager-webhook >/dev/null 2>&1; do echo "    ...waiting for cert-manager deployments to appear"; sleep 5; done'
    if ! kubectl -n cert-manager rollout status deploy/cert-manager --timeout=7m; then
    kubectl -n cert-manager describe deploy cert-manager || true
    kubectl -n cert-manager logs -l app.kubernetes.io/name=cert-manager --tail=200 || true
    log_error_and_exit "Deployment cert-manager failed to roll out."
    fi
    if ! kubectl -n cert-manager rollout status deploy/cert-manager-webhook --timeout=7m; then
    kubectl -n cert-manager describe deploy cert-manager-webhook || true
    kubectl -n cert-manager logs -l app.kubernetes.io/name=webhook --tail=200 || true
    log_error_and_exit "Deployment cert-manager-webhook failed to roll out."
    fi
    log_success "cert-manager core Deployments are Available."

    # 4) ÂÜçÂÅö‰∏ÄÊ¨° apiserver Â∞±Áª™Èó®ÊéßÔºàwebhook/CRD ÂÆâË£ÖÂêéÂ∏∏ËßÅÊ≥¢Âä®Ôºâ
    wait_apiserver_ready 300 5

    # 5) ‰ªé Argo ËßÜËßíÁ≠âÂæÖ cert-manager Application HealthyÔºàÂª∂ÈïøË∂ÖÊó∂‰ª•ÈÄÇÂ∫îÈ¶ñÊ¨°ÂÆâË£ÖÔºâ
    log_info "Waiting for Cert-Manager application to become Healthy in Argo CD..."
    if ! run_with_retry "kubectl get application/cert-manager -n argocd -o jsonpath='{.status.health.status}' | grep -q 'Healthy'" "Cert-Manager Argo CD App to be Healthy" 100 10; then
    kubectl get application/cert-manager -n argocd -o yaml || true
    kubectl -n cert-manager get pods -o wide || true
    kubectl -n cert-manager get events --sort-by=.lastTimestamp | tail -n 50 || true
    kubectl -n argocd get events --sort-by=.lastTimestamp | tail -n 50 || true
    log_error_and_exit "Cert-Manager deployment via Argo CD failed (not Healthy within timeout)."
    fi
    log_success "Cert-Manager application is Healthy in Argo CD."

    log_info "Applying remaining Applications (excluding n8n)..."
    # frps Áã¨Á´ãÁÆ°ÁêÜ
    kubectl apply -f kubernetes/apps/frps-app.yaml
    # core-manifests ‰ªÖÂåÖÂê´ cluster-issuer
    kubectl apply -f kubernetes/apps/core-manifests-app.yaml
    # Êñ∞Â¢û‰∏§‰∏™ÈùôÊÄÅ ingress Â∫îÁî®
    kubectl apply -f kubernetes/apps/argocd-ingress-app.yaml

    # Êñ∞Â¢ûÔºöprovisioner ÁΩëÂÖ≥ Application
    kubectl apply -f kubernetes/apps/provisioner-app.yaml

    kubectl apply -f kubernetes/apps/authentik-ingress-static-app.yaml

    # ÈÄê‰∏™Á≠âÂæÖ HealthyÔºàÊîæÂÆΩË∂ÖÊó∂‰ª•ÈÄÇÂ∫îÈ¶ñÊ¨°Á≠æÂèë/ÊãâËµ∑Ôºâ
    log_info "Waiting for core-manifests application to become Healthy..."
    if ! run_with_retry "kubectl get application/core-manifests -n argocd -o jsonpath='{.status.health.status}' | grep -q 'Healthy'" "core-manifests Argo CD App to be Healthy" 600 10; then
    kubectl get application/core-manifests -n argocd -o yaml || true
    log_error_and_exit "core-manifests not Healthy."
    fi

    log_info "Waiting for argocd-ingress application to become Healthy..."
    if ! run_with_retry "kubectl get application/argocd-ingress -n argocd -o jsonpath='{.status.health.status}' | grep -q 'Healthy'" "argocd-ingress Argo CD App to be Healthy" 600 10; then
    kubectl get application/argocd-ingress -n argocd -o yaml || true
    log_error_and_exit "argocd-ingress not Healthy."
    fi

    # Á≠âÂæÖ provisioner HealthyÔºàËØÅ‰π¶/Ingress ÂàõÂª∫ÂèØËÉΩÁï•ÊÖ¢ÔºåÊîæÂÆΩË∂ÖÊó∂Ôºâ
    log_info "Waiting for provisioner application to become Healthy..."
    if ! run_with_retry "kubectl get application/provisioner -n argocd -o jsonpath='{.status.health.status}' | grep -q 'Healthy'" "provisioner Argo CD App to be Healthy" 600 10; then
    kubectl get application/provisioner -n argocd -o yaml || true
    kubectl -n provisioner get pods -o wide || true
    kubectl -n provisioner get events --sort-by=.lastTimestamp | tail -n 50 || true
    log_error_and_exit "provisioner not Healthy."
    fi
    log_success "provisioner application is Healthy."

    log_info "Waiting for authentik-ingress-static application to become Healthy..."
    if ! run_with_retry "kubectl get application/authentik-ingress-static -n argocd -o jsonpath='{.status.health.status}' | grep -q 'Healthy'" "authentik-ingress-static Argo CD App to be Healthy" 600 10; then
    kubectl get application/authentik-ingress-static -n argocd -o yaml || true
    log_error_and_exit "authentik-ingress-static not Healthy."
    fi

    log_success "Remaining applications submitted and Healthy."
}

function final_verification() {
    log_step 6 "Final End-to-End Verification"
    wait_apiserver_ready 180 5

    log_info "Verifying ClusterIssuer 'cloudflare-staging' is ready..."
    if ! run_with_retry "kubectl wait --for=condition=Ready clusterissuer/cloudflare-staging --timeout=2m" "ClusterIssuer to be Ready" 120 10; then
        log_info "ClusterIssuer did not become ready. Dumping Cert-Manager logs:"
        kubectl logs -n cert-manager -l app.kubernetes.io/instance=cert-manager --all-containers
        log_error_and_exit "ClusterIssuer verification failed."
    fi

    log_info "Verifying ArgoCD IngressRoute certificate has been issued..."
    if ! run_with_retry "kubectl wait --for=condition=Ready certificate/argocd-server-tls-staging -n argocd --timeout=5m" "Certificate to be Ready" 300 15; then
        log_info "Certificate did not become ready. Dumping Cert-Manager logs and describing Certificate:"
        kubectl logs -n cert-manager -l app.kubernetes.io/instance=cert-manager --all-containers
        kubectl describe certificate -n argocd argocd-server-tls-staging
        log_error_and_exit "Certificate issuance failed."
    fi

    log_info "Performing final reachability check on ArgoCD URL: https://${ARGOCD_FQDN}"
    local check_cmd="curl -k -L -s -o /dev/null -w '%{http_code}' --resolve ${ARGOCD_FQDN}:443:${VPS_IP} https://${ARGOCD_FQDN}/ | grep -q '200'"
    if ! run_with_retry "${check_cmd}" "ArgoCD UI to be reachable (HTTP 200 OK)" 120 10; then
        log_info "ArgoCD UI is not reachable or not returning HTTP 200. Dumping Traefik and Argo CD Server logs:"
        kubectl logs -n kube-system -l app.kubernetes.io/name=traefik
        kubectl logs -n argocd -l app.kubernetes.io/name=argocd-server
        log_error_and_exit "End-to-end verification failed."
    fi

    log_info "Verifying Provisioner portal Certificate has been issued..."
    if ! run_with_retry "kubectl wait --for=condition=Ready certificate/portal-tls-staging -n provisioner --timeout=5m" "Portal Certificate to be Ready" 300 15; then
    kubectl -n provisioner describe certificate portal-tls-staging || true
    kubectl -n cert-manager logs -l app.kubernetes.io/instance=cert-manager --all-containers --tail=100 || true
    log_error_and_exit "Portal certificate issuance failed."
    fi

    # [ADD START] wait for provisioner gateway backend readiness and TLS secret presence
    log_info "Waiting for provisioner-gateway Deployment rollout..."
    if ! run_with_retry "kubectl -n provisioner rollout status deploy/provisioner-gateway --timeout=60s" "provisioner-gateway rollout to complete" 240 10; then
    kubectl -n provisioner describe deploy provisioner-gateway || true
    kubectl -n provisioner get pods -o wide || true
    log_error_and_exit "provisioner-gateway failed to roll out."
    fi

    log_info "Waiting for Service/provisioner-gateway endpoints to be populated..."
    if ! run_with_retry "kubectl -n provisioner get endpoints provisioner-gateway -o jsonpath='{.subsets[0].addresses[0].ip}' | grep -E '.+'" "provisioner-gateway Endpoints to be Ready" 240 10; then
    kubectl -n provisioner get endpoints provisioner-gateway -o yaml || true
    log_error_and_exit "provisioner-gateway Endpoints not ready."
    fi

    log_info "Verifying TLS Secret 'portal-tls-staging' exists for Traefik..."
    if ! run_with_retry "kubectl -n provisioner get secret portal-tls-staging >/dev/null 2>&1" "Secret portal-tls-staging available" 180 10; then
    kubectl -n provisioner get secret || true
    log_error_and_exit "TLS Secret 'portal-tls-staging' is missing."
    fi

    # Give Traefik a short window to pick up the secret and router
    log_info "Allowing Traefik to resync TLS assets..."
    sleep 10
    # [ADD END]

    log_info "Performing reachability check on Portal URL: https://${PORTAL_FQDN}"
    # ‰ª• 200/3xx ‰∏∫ÊàêÂäüÔºàecho-server ÈªòËÆ§ 200Ôºâ
    if ! run_with_retry "curl -k -s -o /dev/null -w '%{http_code}' --resolve ${PORTAL_FQDN}:443:${VPS_IP} https://${PORTAL_FQDN}/ | egrep -q '^(200|30[12])$'" "Provisioner portal to be reachable (HTTP 200/30x)" 180 10; then
    kubectl -n kube-system logs -l app.kubernetes.io/name=traefik --tail=120 || true
    kubectl -n provisioner logs deploy/provisioner-gateway --tail=200 || true
    log_error_and_exit "Portal end-to-end verification failed."
    fi
    log_success "Portal is reachable with valid TLS."

    # --- New: frps entryPoint + wildcard TLS verification ---
    verify_frps_entrypoint_and_tls
}

# --- [SECTION 4: MAIN EXECUTION] ---
main() {
    # Pre-flight checks
    if [[ $EUID -ne 0 ]]; then log_error_and_exit "This script must be run as root."; fi
    if ! command -v docker &> /dev/null || ! systemctl is-active --quiet docker; then log_error_and_exit "Docker is not installed or not running."; fi
    if ! command -v helm &> /dev/null; then log_error_and_exit "Helm is not installed. Please install Helm to proceed."; fi
    if [ ! -d "kubernetes/bootstrap" ] || [ ! -d "kubernetes/apps" ]; then log_error_and_exit "Required directories 'kubernetes/bootstrap' and 'kubernetes/apps' not found. Run from repo root."; fi
    
    touch "${LOG_FILE}" &>/dev/null || { echo "FATAL ERROR: Cannot write to log file at ${LOG_FILE}." >&2; exit 1; }
    exec &> >(tee -a "$LOG_FILE")

    log_info "Deployment Bootstrapper (v23.1) initiated. Full log: ${LOG_FILE}"

    ensure_cloudflare_wildcard_a
    perform_system_cleanup
    deploy_etcd
    install_k3s
    bootstrap_gitops
    deploy_applications
    final_verification

    # --- [START OF PASSWORD FIX] ---
    # The password is now static. The success message is updated to reflect this.
    # The 'argocd-initial-admin-secret' should no longer exist with this new method.
    log_info "Verifying 'argocd-initial-admin-secret' is not present..."
    if kubectl get secret argocd-initial-admin-secret -n argocd &>/dev/null; then
        log_warn "The 'argocd-initial-admin-secret' still exists, which is unexpected. The password should be managed by 'argocd-secret'."
    else
        log_success "'argocd-initial-admin-secret' is not present, as expected."
    fi
    # --- [END OF PASSWORD FIX] ---

    echo -e "\n\n\033[1;32m##############################################################################\033[0m"
    echo -e "\033[1;32m#          ‚úÖ DEPLOYMENT COMPLETED SUCCESSFULLY ‚úÖ                         #\033[0m"
    echo -e "\033[1;32m##############################################################################\033[0m"
    echo -e "\nYour personal cluster is ready and managed by ArgoCD."
    echo -e "\n\033[1;33mArgoCD Access Details:\033[0m"
    echo -e "  UI:      \033[1;36mhttps://${ARGOCD_FQDN}\033[0m"
    echo -e "           (NOTE: You must accept the 'staging' or 'untrusted' certificate in your browser)"
    echo -e "  User:    \033[1;36madmin\033[0m"
    echo -e "  Password:\033[1;36m ${ARGOCD_ADMIN_PASSWORD}\033[0m"

    echo -e "\nTo log in via CLI:"
    echo -e "  \033[0;35margocd login ${ARGOCD_FQDN} --username admin --password '${ARGOCD_ADMIN_PASSWORD}' --insecure\033[0m"
    echo -e "\nKubeconfig is available at: ${USER_KUBECONFIG_PATH}"
}

main "$@"


